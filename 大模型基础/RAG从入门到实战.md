<h1 id="目录">目录</h1>

- [1.什么是RAG？](#1什么是RAG？)
- [2.RAG项目中为什么倾向于选择Markdown格式的文档？](#2.RAG项目中为什么倾向于选择Markdown格式的文档？)
- [3.RAG项目中文本分块策略介绍？](#3.RAG项目中文本分块策略介绍？)


<h3 id='1.什么是RAG？'>1.什么是RAG？</h3>

## 一、什么是 RAG？
RAG 全称 Retrieval-Augmented Generation，翻译成中文是检索增强生成。检索指的是检索外部知识库，增强生成指的是将检索到的知识送给大语言模型以此来优化大模型的生成结果，使得大模型在生成更精确、更贴合上下文答案的同时，也能有效减少产生误导性信息的可能。

## 二、为什么需要 RAG？
之所以需要 RAG，是因为大语言模型本身存在一些局限性。

1) 时效性
模型的训练是基于截至某一时间点之前的数据集完成的。这意味着在该时间点之后发生的任何事件、新发现、新趋势或数据更新都不会反映在模型的知识库中。例如，我的训练数据在 2023 年底截止，之后发生的事情我都无法了解。另外，大型模型的训练涉及巨大的计算资源和时间。这导致频繁更新模型以包括最新信息是不现实的，尤其是在资源有限的情况下。

2) 覆盖性
虽然大模型的训练数据集非常庞大，但仍可能无法涵盖所有领域的知识或特定领域的深度信息。例如，某些专业的医学、法律或技术问题可能只在特定的文献中被详细讨论，而这些文献可能未被包括在模型的训练数据中。另外，对于一些私有数据集，也是没有被包含在训练数据中的。当我们问的问题的答案没有包含在大模型的训练数据集中时，这时候大模型在回答问题时便会出现幻觉，答案也就缺乏可信度。

由于以上的一些局限性，大模型可能会生成虚假信息。为了解决这个问题，需要给大模型外挂一个知识库，这样大模型在回答问题时便可以参考外挂知识库中的知识，也就是 RAG 要做的事情。

## 三、RAG 的流程
RAG 的中文名称是检索增强生成，从字面意思来理解，包含三个检索、增强和生成三个过程。

- 检索： 根据用户的查询内容，从外挂知识库获取相关信息。具体来说，就是将用户的查询通过嵌入模型转换成向量，以便与向量数据库中存储的知识相关的向量进行比对。通过相似性搜索，从向量数据库中找出最匹配的前 K 个数据。

- 增强： 将用户的查询内容和检索到的相关知识一起嵌入到一个预设的提示词模板中。

- 生成： 将经过检索增强的提示词内容输入到大语言模型（LLM）中，以此生成所需的输出。 流程图如下所示：
![rag流程图](./imgs/rag流程图.png)


<h3 id='2.RAG项目中为什么倾向于选择Markdown格式的文档？'>2.RAG项目中为什么倾向于选择Markdown格式的文档？</h3>

在RAG（Retrieval-Augmented Generation）系统中，将文档抽取为Markdown格式具有多重优势，主要体现在以下几个方面：
 
1) 结构化与可读性优势
Markdown凭借其简洁的语法和良好的可读性，成为RAG数据处理的理想格式。其一致的格式标准和易于转换的特性为后续的文本处理、分块和向量化提供了便利。相比于复杂的文档格式（如PDF、Word），Markdown能够以最简化的形式保留核心内容结构，同时便于机器解析和人类理解。
 
2) 优化文档分块与检索效果
在RAG系统中，文档分块的质量直接影响检索效果。Markdown格式天然支持通过标题层级（#、##、###等）将文档划分为语义连贯的段落。这种结构化的分块方式能够：
- 保持内容的上下文完整性
- 提高检索结果的相关性
- 为后续的生成阶段提供更精准的上下文信息
 
3) 保留关键文档结构
Markdown能够有效处理表格、代码块、列表等复杂元素，同时保持数据之间的逻辑关系。先进的文档转换工具可以将PDF等复杂格式转换为Markdown，同时保留原始布局、格式和元数据，确保信息在转换过程中不丢失。
 
4) 格式统一与系统兼容性
RAG系统通常需要处理多种来源、多种格式的文档。将所有文档统一转换为Markdown格式，可以：
- 简化预处理流程
- 提高系统的可维护性
- 确保不同文档在向量化和检索时具有一致的处理标准
 
这种偏好与大模型输出为markdown格式有关，主要体现在以下两个层面：
 
1) 输入-输出格式一致性
大型语言模型（LLM）在RAG系统中既需要处理输入的上下文信息，也需要生成最终的输出结果。当输入文档以Markdown格式提供时：
- LLM能够更准确地理解文档结构和语义
- 模型在生成响应时可以自然地采用相同的格式规范
- 保持整个流程（输入→处理→输出）的格式一致性，减少格式转换带来的信息损失
 
2) LLM友好的格式设计
Markdown被广泛认为是"LLM友好型"格式，原因包括：
- 简洁性：没有复杂的样式和布局干扰，让模型专注于内容本身
- 标准化：统一的语法规范降低了模型的理解难度
- 结构化：标题、列表、表格等元素为模型提供了明确的语义标记
- 通用性：几乎所有现代LLM都经过大量Markdown格式数据的训练，对此格式具有天然的适应性


<h3 id='3.RAG项目中文本分块策略介绍？'>3.RAG项目中文本分块策略介绍？</h3>

## 一、基本概念介绍
1. 什么是RAG中的文本分块（Text Chunking）？
文本分块是将庞大的原始文本（如长篇报告、电子书、API文档等）分割成更小、更易于处理的文本片段（Chunks）的过程。这些Chunks是RAG系统信息处理的基本单元，会被送入Embedding模型向量化后存入向量数据库，最终服务于检索环节。
 
2. 为什么RAG系统离不开文本分块？核心目标是什么？
文本分块是RAG系统的核心前置步骤，核心目标有三点：
- 克服上下文窗口限制：大语言模型（LLM）存在上下文长度限制，分块能确保输入LLM的信息在其“消化能力”范围内；
- 提高检索精度与效率：小而语义集中的块可减少无关信息干扰，提升匹配精度，同时向量数据库检索小块的速度远快于全文搜索；
- 维护上下文完整性：理想分块需尽可能保持语义连贯，避免割裂完整逻辑（如句子、代码块）。
 
3. 文本分块对RAG性能有哪些深远影响？
分块策略直接决定RAG系统的检索质量和生成质量：
- 检索质量：分块的粒度、边界确定方式、块间关联（如是否重叠），决定检索器能否准确找到相关片段。糟糕分块会导致返回不相关、不完整或冗余信息；
- 生成质量：LLM的输出质量依赖检索到的Chunks，若Chunks上下文割裂、信息缺失，即使LLM能力再强，也难以生成准确连贯的答案。
 
4. 文本分块的核心挑战是什么？
核心挑战是“检索精度（Precision）”与“上下文完整性（Context）”的权衡：
- 小块优势：信息聚焦、语义集中，检索精度高；缺点：可能丢失上下文，无法支撑复杂问题解答；
- 大块优势：保留丰富上下文，利于理解复杂概念；缺点：包含较多噪音，稀释相关性信号，降低检索精度，增加LLM处理负担。

## 二、基础分块策略
1. 固定大小分块（Fixed-size Chunking）的核心逻辑、优缺点及适用场景是什么？
- 核心逻辑：按固定字符数/Token数切割，可设置“重叠（Overlap）”部分缓解语义割裂（如chunk_size=500字符，chunk_overlap=50字符）；
- 优点：实现简单、计算开销小、对文本格式无要求；
- 缺点：易破坏语义完整性（如句子/单词中间切割）、忽略文本结构；
- 适用场景：对结构要求低的简单场景、海量数据快速预处理、复杂策略的兜底手段。
 
2. 基于句子的分块（Sentence Splitting）有何特点？适合处理哪种文本？
- 核心逻辑：先通过标点或NLP库（NLTK、SpaCy）分割句子，再将连续句子合并成接近目标大小的Chunks；
- 优点：保持句子级语义完整性，符合自然语言结构；
- 缺点：句子长度差异大导致Chunks大小不均，简单标点分割可能误判（如Mr. Smith中的“.”），对无句子结构文本（如代码、JSON）效果差；
- 适用场景：结构良好的文本（新闻、报告、小说），需保持句子语义完整的场景。
 
3. 递归字符分块（Recursive Character Text Splitting）的优势的是什么？
- 核心逻辑：按预设分隔符优先级递归分割（如优先\n\n（段落）→\n（换行）→空格→字符），确保块大小不超限；
- 优点：兼顾语义结构与大小控制，比固定大小更智能，适应性强；
- 缺点：实现较复杂，效果依赖分隔符优先级设计，无明显分隔符时退化为字符分割；
- 适用场景：多种文本文档，需控制块大小且保留文本结构的场景（常作为RAG框架默认选项）。
 
4. 基于文档结构的分块（Document Structure-aware Chunking）如何工作？有何特点？
- 核心逻辑：利用文档固有结构分割（如HTML标签、Markdown标题/列表、JSON层级），如每个<p>标签或Markdown二级标题下内容作为一个Chunk；
- 优点：尊重原文结构、语义连贯性强，可附加结构元数据（如标题）辅助检索；
- 缺点：依赖清晰文档结构，结构元素文本量差异大导致Chunks大小不均，需针对不同格式编写解析逻辑；
- 适用场景：结构化文档（网页、Markdown、JSON/XML），需利用结构信息检索的场景。
 
5. 混合分块（Hybrid Chunking）的核心思路和优势是什么？
- 核心思路：组合多种策略，先按文档结构（如Markdown标题）粗粒度分割，再对超大小的块用递归字符/句子分块细切，保留结构元数据；
- 优点：兼顾结构完整性与大小控制，元数据丰富，灵活性高；
- 缺点：实现复杂度高，需设计组合逻辑和参数；
- 适用场景：对分块质量要求高的场景（如Markdown、富文本文档），需平衡上下文、结构与大小的需求。

## 三、进阶分块策略
1. 语义分块（Semantic Chunking）与基础策略的本质区别是什么？
- 核心逻辑：不依赖字符/标点，通过计算相邻文本的Embedding向量相似度，在语义断裂点（相似度低于阈值）切割；
- 优点：切分点贴合语义，Chunk内部语义高度相关，符合人类阅读理解习惯；
- 缺点：计算开销大（需生成Embedding），依赖Embedding模型能力，阈值需实验调优；
- 适用场景：分块质量要求高、计算资源充裕的场景，如无结构化标记的长文本（纯文本、对话记录）。
 
2. 分层分块（Hierarchical Chunking）的核心设计是什么？
- 核心逻辑：系统化创建多层级Chunks（如章节→段落→句子），不同层级Chunks可分别索引，适配不同检索需求；
- 优点：提供多粒度上下文，增加检索灵活性；
- 缺点：增加索引复杂度和存储空间，需设计层级关系；
- 适用场景：复杂层级文档（书籍、长篇报告），需灵活选择上下文粒度的检索场景。
 
3. Small-to-Big检索（父文档检索器）如何结合分块提升效果？
- 核心逻辑：依赖分层/父子关系分块，检索流程为：用查询匹配小块（保证精度）→返回小块所属的父块（提供完整上下文）；
- 优点：结合小块的检索精度和大块的上下文完整性，兼顾精准定位与背景支撑；
- 缺点：需维护小块与父块的映射关系，增加索引和检索逻辑复杂度；
- 适用场景：需高精度检索且需丰富上下文生成答案的RAG应用。
 
4. 命题分块（Proposition Chunking）适合哪些场景？
- 核心逻辑：通过LLM或NLP模型提取文本中的原子性事实命题（如“苹果2023年发布Vision Pro”拆分为3个独立命题）；
- 优点：细粒度、高聚焦，适合精确事实检索；
- 缺点：依赖模型抽取能力，计算成本高，可能丢失文本语气和复杂关系；
- 适用场景：知识库构建、事实性问答系统，对信息原子性和精确性要求极高的场景。
 
5. Agentic/LLM-based Chunking的核心思路和现状是什么？
- 核心逻辑：让Agent或LLM通过Prompt决策分块方式，动态选择组合策略；
- 优点：理论上可实现最智能的语义化分块；
- 缺点：实现复杂（需Prompt工程/Agent设计），成本高、速度慢，结果可控性差；
- 适用场景：研究探索项目，对分块质量有极致追求且不计成本的应用。

## 四、块优化策略
1. 如何通过上下文富化（Context Enrichment）优化分块效果？
- 核心思路：分块后为Chunk添加额外上下文（如相邻句子、所属章节标题、摘要等元数据）；
- 优点：不显著增大Chunk大小，帮助LLM理解Chunk在原文中的位置和背景；
- 缺点：需额外处理步骤提取富化信息；
- 适用场景：可与任意分块策略结合，尤其适合小块分块（弥补上下文不足）。
 
2. 选择分块策略时需考虑哪些因素？
需结合数据特性和应用场景综合判断：
- 数据特性：文本类型（结构化/非结构化）、结构复杂度、信息密度、语言类型；
- 应用场景：检索目标（精确事实/复杂概念）、响应速度要求、计算资源、LLM上下文窗口大小。
 
3. 评估分块策略好坏的关键指标有哪些？
除了Chunk长度分布，还包括：
- 检索指标：精度（相关Chunk命中率）、召回率（是否覆盖所有相关信息）；
- 生成指标：LLM回答的准确性、连贯性、完整性；
- 效率指标：分块处理速度、检索响应速度、存储开销。