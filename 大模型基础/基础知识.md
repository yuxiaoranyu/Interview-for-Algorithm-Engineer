<h1 id="目录">目录</h1>

- [1.LLM中token指的是什么？](#1.LLM中token指的是什么？)
- [2.哪些因素会导致LLM中的偏见？](#2.哪些因素会导致LLM中的偏见？)
- [3.如何减轻LLM中的“幻觉”现象？](3.如何减轻LLM中的“幻觉”现象？)
- [4.解释一下大模型的涌现能力？](#4.解释一下大模型的涌现能力？)
- [5.解释一下MOE，它的作用主要是什么？](#5.解释一下MOE，它的作用主要是什么？)
- [6.如何缓解大语言模型inference时候重复的问题？](#6.如何缓解大语言模型inference时候重复的问题？)
- [7.什么是大模型智能体？](#7.什么是大模型智能体？)
- [8.LLM有哪些类型？](#8.LLM有哪些类型？)
- [9.什么是基础模型？什么是开源模型，和闭源模型？](9.什么是基础模型？什么是开源模型，和闭源模型？)
- [10.什么是语言模型？](#10.什么是语言模型？)
- [11.什么是自回归语言模型？](#11.什么是自回归语言模型？)
- [12.什么是信息理论？](#12.什么是信息理论？)
- [13.什么是n-gram模型？](#13.什么是n-gram模型？)
- [14.大语言模型的应用风险有哪些？](#14.大语言模型的应用风险有哪些？)
- [15.什么是大语言模型的适应性？](#15.什么是大语言模型的适应性？)
- [16.语言模型有哪些分类？](#16.语言模型有哪些分类？)
- [17.什么是注意力机制？](#17.什么是注意力机制？)
- [18.什么是语言模型的“两类错误”及其影响?](#18.什么是语言模型的“两类错误”及其影响?)
- [19.有哪些常见的语言任务?](#19.有哪些常见的语言任务?)
- [20.什么是分词?](#20.什么是分词?)
- [21.什么是最大匹配算法?](#21.什么是最大匹配算法?)
- [22.如何解决模型规模过大导致的难以扩展问题？](#22.如何解决模型规模过大导致的难以扩展问题？)
- [23.什么是混合专家模型？](#23.什么是混合专家模型？)
- [24.怎么构建大模型领域的数据集？](#24.怎么构建大模型领域的数据集？)
- [25.Decoder-only模型训练的目标函数是什么？](#25.Decoder-only模型训练的目标函数是什么？)
- [26.Encoder-only模型训练的目标函数是什么？](#26.Encoder-only模型训练的目标函数是什么？)
- [27.Encoder-decoder模型训练的目标函数是什么？](#27.Encoder-decoder模型训练的目标函数是什么？)
- [28.优化算法怎么应用在大模型的训练中？](#28.优化算法怎么应用在大模型的训练中？)
- [29.什么是大模型的混合精度训练？](#29.什么是大模型的混合精度训练？)
- [30.Probing方法怎么用于下游任务的迁移？](#30.Probing方法怎么用于下游任务的迁移？)
- [31.Prompt Tuning方法怎么用于下游任务的迁移？](#31.PromptTuning方法怎么用于下游任务的迁移？)
- [32.PrefixDecoder/CausalDecoder/Encoder-Decoder的区别有哪些?](#32.PrefixDecoder/CausalDecoder/Encoder-Decoder的区别有哪些?)
- [33.当前优化模型最主要技术手段有哪些?](#33.当前优化模型最主要技术手段有哪些?)
- [34.大模型推理加速框架有哪一些?都有什么特点?](#34.大模型推理加速框架有哪一些?都有什么特点?)
- [35.大语言模型命名中7B、13B、540B是什么意思？](#35.大语言模型命名中7B、13B、540B是什么意思？)
- [36.为什么现在的大模型结构大部分是Decoder only结构?](#36.为什么现在的大模型结构大部分是Decoderonly结构?)
- [37.目前各LLMs 都使用哪种激活函数?](#37.目前各LLMs都使用哪种激活函数?)
- [38.介绍一下FFN块计算公式](#38.介绍一下FFN块计算公式)
- [39.进行SFT操作的时候，基座模型选用Chat还是Base?](#39.进行SFT操作的时候，基座模型选用Chat还是Base?)
- [40.如果想要在某个模型基础上做全参数微调，需要多少显存?如何计算？](#40.如果想要在某个模型基础上做全参数微调，需要多少显存?如何计算？)
- [41.为什么SFT之后感觉LLM傻了?](#41.为什么SFT之后感觉LLM傻了?)
- [42.领域模型Continue PreTrain 数据选取?](#42.大语言模型命名中7B、13B、540B是什么意思？)
- [43.领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力?](#43.领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力?)
- [44.什么是分布式训练？](#44.什么是分布式训练？)
- [45.大模型为什么需要分布式训练？](#45.大模型为什么需要分布式训练？)
- [46.什么是数据并行策略？](#46.什么是数据并行策略？)
- [47.什么是数据并行策略中的AllReduce操作？](#47.什么是数据并行策略中的AllReduce操作？)
- [48.什么是模型并行策略？](#48.什么是模型并行策略？)
- [49.什么是流水并行策略？](#49.什么是流水并行策略？)
- [50.什么是混合并行策略？](#50.什么是混合并行策略？)
- [51.什么是大模型的有害性（危害）？](#51.什么是大模型的有害性（危害）？)
- [52.大模型中的优化算法有哪些常见的选择？它们各有什么优缺点？？](#52.大模型中的优化算法有哪些常见的选择？它们各有什么优缺点？)
- [53.描述评估LLM性能的一些技术。](#53.描述评估LLM性能的一些技术。)
- [54.介绍一下LLM使用中的一些伦理考虑](#54.介绍一下LLM使用中的一些伦理考虑)
- [55.LLM如何处理超出领域或无意义的提示？](#55.LLM如何处理超出领域或无意义的提示？)
- [56.在现实项目应用中，大规模部署LLM有哪些挑战？](#56.在现实项目应用中，大规模部署LLM有哪些挑战？)
- [57.简单介绍一下LLM在广泛的人工通用智能（AGI）领域中的作用](#57.简单介绍一下LLM在广泛的人工通用智能（AGI）领域中的作用)
- [58.prefix LM 和 causal LM 区别是什么？](#58.prefixLM和causalLM区别是什么？)
- [59.什么是LLMs的复读机现象？](#59.什么是LLMs的复读机现象？)
- [60.为什么会出现复读机现象？](#60.为什么会出现复读机现象？)
- [61.如何缓解LLMs复读机现象？](#61.如何缓解LLMs复读机现象？)
- [62.LLMs是否对输入本文的长度有限制？](#62.LLMs是否对输入本文的长度有限制？)
- [63.如何让大模型处理更长的文本？](#63.如何让大模型处理更长的文本？)
- [64.为什么需要领域的垂直模型？](#64.为什么需要领域的垂直模型？)
- [65.中文分词有什么难点？](#65.中文分词有什么难点？)
- [66.什么是基于词典的分词算法？](#66.什么是基于词典的分词算法？)
- [67.什么是基于统计的分词算法？](#67.什么是基于统计的分词算法？)
- [68.什么是词性标注？难点在哪里？](#68.什么是词性标注？难点在哪里？)
- [69.什么是激活函数？什么是梯度爆炸和梯度消失？](#69.什么是激活函数？什么是梯度爆炸和梯度消失？)
- [70.什么句法分析？](#70.什么是句法分析？)
- [71.什么是词向量？](#71.什么是词向量？)
- [72.为什么Decoder-only架构成为了大模型的主要框架？](#72.为什么Decoder-only架构成为了大模型的主要框架？)
- [73.有哪些常见的语言特征抽取网络？](#73.有哪些常见的语言特征抽取网络？)
- [74.什么是BERT模型？](#74.什么是BERT模型？)
- [75.什么是Word2Vec工具？](#75.什么是Word2Vec工具？)
- [76.Word2Vec中为什么使用负采样？](#76.Word2Vec中为什么使用负采样？)
- [77.什么是Distributed representation?](#77.什么是Distributedrepresentation?)
- [78.你能否概括介绍一下 ChatGPT 的训练过程？](#78.你能否概括介绍一下ChatGPT的训练过程？)
- [79.大模型中的Reasoning和Inference有什么区别？](#79.大模型中的Reasoning和Inference有什么区别？)
- [80.大模型中外推技术是什么？](#80.大模型中外推技术是什么？)
- [81.大模型中输入token过长，超出模型限制如何截断？](#81.大模型中输入token过长，超出模型限制如何截断？)
- [82.什么是DAPO算法？](#82.什么是DAPO算法？)
- [83.什么是VAPO算法？](#83.什么是VAPO算法？)

<h3 id='1.LLM中token指的是什么？'>1.LLM中token指的是什么？</h3>

在大语言模型中，Token是模型进行语言处理的基本信息单元，它可以是一个字，一个词甚至是一个短语句子。Token并不是一成不变的，在不同的上下文中，他会有不同的划分粒度。


<h3 id='2.哪些因素会导致LLM中的偏见？'>2.哪些因素会导致 LLM 中的偏见？</h3>

在大型语言模型（LLM）中，偏见可能来源于多个因素，包括以下几个方面：

1. **训练数据的偏差**：LLM 的性能依赖于所使用的训练数据。如果训练数据中包含偏见（例如，种族、性别、年龄、宗教等方面的偏见），模型可能会在生成文本时反映出这些偏见。

2. **数据选择与采样方法**：如果训练数据在选择和采样过程中不够多样化或不够平衡，可能导致模型对某些群体或观点的偏见。某些少数群体或观点可能在训练数据中被低估或忽视，从而导致模型表现出偏见。

3. **模型架构和训练方法**：虽然模型架构本身并不直接产生偏见，但特定的设计选择和训练方法可能会放大训练数据中的偏见。例如，过度优化某些性能指标（如精度）可能会忽视公平性和多样性。

4. **人类标注者的偏见**：在训练监督学习模型时，标注数据的过程通常涉及人类标注者。如果这些标注者带有偏见，他们的偏见可能会传递到训练数据中，从而影响模型的输出。

5. **模型部署和使用环境**：即使模型在训练过程中没有明显偏见，在实际部署和使用过程中，用户交互和反馈也可能引入新的偏见。例如，某些用户输入可能会导致模型生成偏见性回答。

6. **社会和文化背景**：语言和文化是动态变化的，不同社会和文化背景下的语言使用方式不同。如果模型训练数据主要来自特定文化或语言环境，可能会对其他文化或语言产生偏见。

为了减少这些偏见，研究人员和开发者可以采取以下措施：

- **多样化训练数据**：确保训练数据在性别、种族、文化、社会经济背景等方面具有多样性。

- **偏见检测和消除**：使用技术手段检测和消除模型中的偏见，例如通过去偏算法和公平性评估工具。

- **透明度和解释性**：增加模型的透明度，使用户能够理解模型的决策过程，并及时识别和纠正偏见。

- **持续监控和改进**：在模型部署后持续监控其表现，收集用户反馈，并定期更新和改进模型。
这些方法可以帮助减少 LLM 中的偏见，提高其公平性和可靠性。


<h3 id='3.如何减轻LLM中的“幻觉”现象？'>3.如何减轻 LLM 中的“幻觉”现象？</h3>

大模型幻觉问题主要指：指的是模型生成的内容看似合理但实际上是错误或虚构的信息。
减轻大型语言模型（LLM）中的“幻觉”现象可以通过多种方法实现。改进训练数据质量和训练方法，包括数据清洗、监督学习和强化学习，确保数据的准确性和多样性；采用后处理技术，如事实验证和编辑校对，确保生成内容的真实性；改进模型架构，结合外部知识库和多任务学习增强模型对事实的理解；提高模型透明度和可解释性，使用户能够理解和检查模型的输出；建立用户教育和反馈机制，鼓励用户验证生成内容并报告错误；以及定期更新和维护模型和数据。通过这些方法，可以显著减少模型生成错误信息的可能性，提高内容的准确性和可靠性。


<h3 id='4.解释一下大模型的涌现能力？'>4.解释一下大模型的涌现能力？</h3>

大模型的涌现能力指的是，当模型的规模和复杂度达到一定程度时，出现了一些在较小模型中未曾观察到的新特性或能力，如语言理解与生成、推理、多语言处理和少样本学习等。这些能力并非通过直接编程实现，而是在大量数据和复杂训练过程中自然涌现的。


<h3 id='5.解释一下MOE，它的作用主要是什么？'>5.解释一下MOE，它的作用主要是什么？</h3>

混合专家模型（Mixture of Experts：MoE）是一种稀疏门控制的深度学习模型，它主要由一组专家模型和一个门控模型组成。MoE的基本理念是将输入数据根据任务类型分割成多个区域，并将每个区域的数据分配一个或多个专家模型。每个专家模型可以专注于处理输入这部分数据，从而提高模型的整体性能。

MoE架构的基本原理非常简单明了，它主要包括两个核心组件：GateNet和Experts。GateNet的作用在于判定输入样本应该由哪个专家模型接管处理。而Experts则构成了一组相对独立的专家模型，每个专家负责处理特定的输入子空间。

微软研究报告，参考链接：
https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/


<h3 id='6.如何缓解大语言模型inference时候重复的问题？'>6.如何缓解大语言模型inference时候重复的问题？</h3>

缓解大语言模型推理时重复问题的方法包括引入重复惩罚机制、多样性采样技术（如温度采样、Top-k采样、Top-p采样）、N-gram去重、改进模型架构和训练方法（如长程记忆机制、训练数据去重）以及生成后的后处理技术。这些策略可以有效减少生成文本中的重复现象，提高生成内容的多样性和连贯性。


<h3 id='7.什么是大模型智能体？'>7.什么是大模型智能体？</h3>

**智能体**是具有自主性、反应性、积极性和社交能力特征的智能实体，由三个部分组成：**控制端（Brain）**，**感知端（Perception**）和**行动端（Action）**。

![Agent](imgs/基础知识/0706-Agent.png)

- **控制端**：主要由大型语言模型（LLMs）组成，负责存储记忆和知识，处理信息，并制定决策。它能够规划任务、理解上下文和知识库，并作为主控激活其他功能。

-  **感知端**：智能体通过这一部分接收外部信息，包括使用自然语言处理技术来理解文本信息，以及利用计算机视觉技术来分析图像和视频数据等。

-  **行动端**：智能体通过这一组件与外部环境互动并产生影响。这包括生成文本和图像、机器人的具身交互能力，以及调用各种工具来完成特定任务。


<h3 id='8.LLM有哪些类型？'>8.LLM有哪些类型？</h3>

LLM大模型根据应用领域的不同，分为文本、音频、视频、图像生成等类型。

- **音频和语音**：大模型直接分析给定的音频，并自动生成所需的音频数据，涵盖多语言语音识别，感情辨识，自然语音生成，多语言翻译等等问题，例如：FunAudioLLM。

- **图像和视频**：根据给定的文本、图像、视频等单模态数据，自动生成符合描述的、高保真的图像和视频内容，涵盖图像和视频的生成，理解，修复以及压缩等问题，例如：DALL-E。

- **文本类型**：  指利用自然语言处理技术，通过对大量文本数据的学习和理解，以及对语言规律的掌握，自动生成符合语法和语义要求的文本内容，涵盖文本和代码的生成，理解，翻译和改写等问题，例如：GPT-4。

- **多模态**： 将自然语言处理与视觉理解，音频处理等其他模态相结合，并通过多模态界面实现交互，实现在输入和输出中处理多种类型的数据，例如：GPT-4o。


<h3 id='9.什么是基础模型？什么是开源模型，和闭源模型？'>9.什么是基础模型？什么是开源模型，和闭源模型？</h3>

**1. 基础模型**：

Foundation model 出自论文[《On the Opportunities and Risks of Foundation Models》](https://arxiv.org/abs/2108.07258?WT.mc_id=academic-105485-koreyst)，其定义标准包括：
	
	（1）使用无监督学习或者自监督学习，仅接受未标记数据的预训练，而没有人工注释或标记数据的参与；
	（2）模型规模很大， 通常超过数十亿的参数；
	（3）作为基座模型，仅需要通过微调即可转变为特定应用模型。

**2.开源模型与闭源模型**

- **开源模型**： 开源模型是对公众开放，任何人都可以使用的模型，允许任何针对LLM的修改和定制，例如：LLaMA模型。

- **闭源模型**： 闭源模型为公司专有仅对公众开放接口的模型，例如：GPT-4o。

<h3 id='10.什么是语言模型？'>10.什么是语言模型？</h3>

语言模型是一种概率模型，用于预测词元（token）序列的概率分布。假设我们有一个词元集 $V$ ，语言模型则是为每个词元序列 $w_{1},...,w_{n} ∈ V$ 预测一个联合概率 $p(w_1, w_2 ... w_n)$ 。通过比较不同词元序列的联合概率，我们可以确定哪个序列在给定上下文中是最可能的，即最佳词元序列。

假设我们有以下三个词元序列，需要确定哪个是最佳序列：

$$
p(\text{你家的, 猫, 吃了, 那只, 老鼠}) = 0.02,
$$

$$
p(\text{那只, 老鼠,吃了, 你家的, 猫}) = 0.01,
$$

$$
p(\text{猫, 你家的, 那只, 老鼠, 吃了}) = 0.0001,
$$

根据语言模型，每个序列都会被赋予一个联合概率。我们来分析这些序列：

1. “你家的猫吃了那只老鼠” - 这个序列既符合语法规则，也符合我们对世界的常识。因此，语言模型会赋予它最高的联合概率。

2. “那只老鼠吃了你家的猫” - 尽管这个序列在语法上是正确的，但它违背了我们对世界的基本常识，因为老鼠通常不会吃猫。因此，语言模型也会赋予它很低的概率。

3. “猫你家的那只老鼠吃了” - 这个序列的语法不正确，主语和谓语的位置混乱，因此语言模型会赋予它很低的概率。

进一步分析，语言模型的任务不仅仅是学习如何为正确的词元序列赋予最高的联合概率，它还需要学习语法规则和世界知识。例如，语言模型应当识别出“猫你家的那只老鼠吃了”这样的序列具有很低的概率，因为该句子的语法结构混乱；同样，模型也应该对“那只老鼠吃了你家的猫”这样的句子赋予较低的概率，因为这违反了我们对世界的基本常识。因此，一个优秀的语言模型应当具备出色的语法理解和世界知识，这样才能更准确地预测词元序列的概率。

语言模型也可以做生成任务。最纯粹的方法是从基于概率的采样，其过程是：从第一个词语或字符开始，根据语言模型给出的概率分布，选择下一个词语或字符，然后基于新的序列，再次使用模型进行预测，选择下一个词语或字符，如此循环，直到生成一个完整的文本序列。

<h3 id='11.什么是自回归语言模型？'>11.什么是自回归语言模型？</h3>

自回归语言模型是一种使用先前的文字来预测下一个文字的模型。其通过逐词地生成文本，每一步都基于之前生成的内容，是通过逐步预测每个位置的单词来生成一句话或一段话的模型。

假设将序列 $𝑥_{1:𝐿}$ 的联合分布为$ 𝑝(𝑥_{1:𝐿})$ ，其常见写法是使用概率的链式法则：
$$
p(x_{1:L})=p(x_1)p(x_2|x_1)...p(x_L|x_{1:L-1})=\prod^L_{i=1}p(x_i|x_{1:i-1})
$$
自回归语言模型的特点是它可以利用前馈神经网络等方法有效计算出每个条件概率分布 $𝑝(𝑥_𝑖∣𝑥_{1:𝑖−1}) $。在自回归语言模型 $𝑝$ 中生成整个序列$ 𝑥_{1:𝐿}$ ，需要一次生成一个token，该token则是基于之前以生成的toke进行计算获得。

例如，要生成一句话“猫吃老鼠”：

1. 模型首先预测第一个词（例如：“猫”）。
2. 然后它使用“猫”来预测下一个词（例如：“吃”）。
3. 接着使用“猫吃”来预测下一个词（例如：“老鼠”）。
4. 迭代这个过程，直到生成完整的句子。

<h3 id='12.什么是信息理论？'>12.什么是信息理论？</h3>

信息理论是研究语言模型的重要理论，其是一门研究信息的度量、传递、存储和处理的学科。它由克劳德·香农（Claude Shannon）在20世纪40年代创立，主要应用于通信、数据压缩、加密、以及编码等领域。信息理论提供了一个框架，用于理解和优化信息系统的性能。

信息理论中最重要的一个概念是信息量（Entropy），也叫信息熵，它是用来度量信息不确定性的一个指标，其公式表达为：
$$
H(X)=-\sum_iP(x_i)\log P(x_i)
$$
其中，$H(X)$为离散随机变量$X$的信息熵，$P(x_i)$是$X$取值$x_i$的概率。

熵的实际上是一个衡量将样本$x\sim p$ 编码成比特串所需要的预期比特数的度量。熵的值越小，表明序列的结构性越强，编码的长度就越短。直观地讲， $-\log⁡ p(𝑥)$ 可以视为用于表示出现概率为$ 𝑝(𝑥)$ 的元素$ 𝑥$ 的编码的长度。

<h3 id='13.什么是n-gram模型？'>13.什么是n-gram模型？</h3>

n-gram模型是一种用于自然语言处理和概率语言建模的基本方法。它通过统计文本中$n$个连续单词出现的频率来预测下一个单词的概率，从而生成或分析文本。n-gram模型广泛应用于文本生成、拼写校正、语音识别和机器翻译等任务。

根据$n$的取值，n-gram可以是单词（$n=1$）、二元组（$n=2$）、三元组（$n=3$）等。例如，对于语句“猫吃老鼠”：

- 当$n=1$时，n-gram为“猫”、“吃”、“老”、“鼠”

- 当$n=2$时，n-gram为“猫吃”、“吃老”、“老鼠”

- 当$n=3$时，n-gram为“猫吃老”、“吃老鼠”

在一个n-gram模型中，关于$x_{i}$的预测只依赖于最后的 $𝑛−1$ 个字符$ 𝑥_{𝑖−(𝑛−1):𝑖−1}$ ，而不是整个历史：
$$
𝑝(𝑥_𝑖∣𝑥_{1:𝑖−1})=𝑝(𝑥_𝑖∣𝑥_{𝑖−(𝑛−1):𝑖−1}).
$$
如果$n$太小，那么模型将难以捕获长距离的依赖关系，下一个词将无法依赖于靠前的单词。然而，如果$n$太大，则统计上将无法得到概率的好估计。

<h3 id='14.大语言模型的应用风险有哪些？'>14.大语言模型的应用风险有哪些？</h3>

- **社会偏见**：大语言模型在处理不同群体的数据时，其表现并不一致，存在一定的偏差。具体来说包含**性能差异**和**刻板印象**两个方面。比如：由于GPT等大模型中亚洲人数据占比较小，所以可能导致在有关亚洲人的问题上性能欠佳或者生成具有刻板印象的答案。
- **有害性**：由于大语言模型是基于大量的互联网数据进行训练，所以可能导致大模型在生成文本时产生有害内容的倾向。
- **虚假信息**：由于大语言模型具备高度的语言生成能力，恶意行为者得以更加便捷地制造语法正确、风格一致且看似可信的虚假新闻，从而加剧了虚假信息宣传的泛滥和危害。
- **安全性**：由于大语言模型是通过抓取和利用网络数据进行训练的，因此任何人都有可能通过篡改或创建有毒数据来影响这些模型的训练过程，进而潜在地攻击和操纵大型模型的行为和输出。
- **法律效应**：如果训练数据中包含了受版权保护的作品，而使用这些作品未经版权持有者的许可，是否造成侵权？如果大语言模型生成的文本与受版权保护的原创作品相似到足以构成实质性相似，是否造成抄袭？
- **成本**：大语言模型的训练和推理过程需要巨大的算力支持，这带来了显著的成本问题：包括训练成本，推理成本和访问成本。

<h3 id='15.什么是大语言模型的适应性？'>15.什么是大语言模型的适应性？</h3>

大语言模型的核心是表示token序列的概率分布，不仅可以用来**评估**一个句子在自然语言中出现的可能性，还可以在给定部分序列（prompt）的情况下，**生成**与之匹配的后续序列，从而创建完整的句子或文本。因此，语言模型可以完成大量的自然语言任务，比如Language Modeling，Question Answering，Arithmetic，news Article Generation, translation, Novel Tasks等。我们使用**“适应”**一词来描述将**通用语言模型**转换为专门针对**特定自然语言任务模型**的过程。

常见的适应技巧包括**监督学习**和**提示学习**。**监督学习**：重新训练或者微调大语言模型。**提示学习**：通过设计并输入特定任务的提示或上下文信息，指导语言模型生成满足这些任务需求的输出。**监督学习**可能因数据问题导致模型过拟合或欠拟合，而**提示学习**则可能因输入提示长度限制而影响生成效果的质量。

<h3 id='16.语言模型有哪些分类？'>16.语言模型有哪些分类？</h3>

根据语言模型的架构，总体可分为编码器（Encoder-only）、解码器（Decoder-only）以及编码器-解码器（Encoder-Decoder）三种架构。

- 编码器架构：这类模型主要专注于理解输入文本的上下文语义信息。模型能够根据输入的文本生成向量表征，表征可用于文本分类等下游任务。该类型的模型典型代表是BERT、RoBERTa等。

- 解码器架构：该模型主要用于生成任务，相较于编码器架构，其能更进一步生成文本，有简单的训练目标。该类型的模型代表有GPT。

- 编码器-解码器架构：该类模型同时包含编码器和解码器，其中编码器负责理解文本输入，解码器负责生成文本。典型的代表是Transformer、BART以及T5模型等。

<h3 id='17.什么是注意力机制？'>17.什么是注意力机制？</h3>

注意力机制（Attention Mechanism）是一种处理序列数据的重要技术，被广泛应用于各种自然语言处理和计算机视觉任务。注意力机制的核心思想是允许模型在处理每个输入或输出元素时，动态地选择和关注输入序列的不同部分，从而更好地捕捉和利用相关信息。

以Transformer模型为例，其编码器和解码器结构中均存在注意力机制。Transformer使用多头注意力机制（Multi-head Attention），即通过多个并行的注意力层来捕捉序列中不同位置的重要关系，从而在处理长距离任务的同时，能够有效地关注最相关的信息。

<h3 id='18.什么是语言模型的“两类错误”及其影响?'>18.什么是语言模型的“两类错误”及其影响?</h3>

- **召回错误**：在自然语言处理中，语言模型在尝试预测文本中的下一个词（token）时，没有准确地估计出该词出现的概率。例如，如果实际文本中的下一个词是“苹果”，但模型预测这个词出现的概率非常低，那么就认为模型未能正确地为“苹果”这个词分配概率。

- **精确度错误**：在预测文本序列时，错误地高估了某些不合适或不符合实际语境的序列出现的概率。例如，在一个关于天气的对话中，如果模型预测出“今天天气很好，我去游泳，然后吃了一块月亮”，这里的“吃了一块月亮”就是一个错误的词序列。

当出现**召回错误**时，**困惑度**会非常高，因为模型未能预测到实际出现的词，这相当于模型给实际词分配了**接近于零的概率**。在几何平均的计算中，任何接近零的概率都会极大地增加整个序列的困惑度，因为几何平均对极端低概率非常敏感。但对于**精确度错误**，困惑度只会进行适度惩罚。 因为实际词的概率降低不大，导致其增量大概和错误序列占总文本的比例相当。

<h3 id='19.有哪些常见的语言任务?'>19.有哪些常见的语言任务?</h3>

**语言建模 任务**是预测文本序列中下一个token的任务。该任务基于给定的部分序列，预测下一个最可能的token，旨在捕捉自然语言的统计特性。该任务的概率可以通过链式规则表示为： $p(x_{1:l})=\prod_{i=1}^l{p(x_i|x_{1:i-1}})$。相关数据集有 Penn Tree Bank，LAMBADA，HellaSwag 等

**问答任务**是解决闭卷问答题的任务，其中输入是一个问题，输出是一个答案。语言模型必须以某种方式“知道”答案，而无需在数据库或一组文档中查找信息。相关数据集有：TriviaQA，WebQuestions

**翻译任务**涉及将源语言（如德语）的句子转换为目标语言（如英语）的句子。这一过程经历了从统计机器翻译到神经机器翻译的发展，再到利用大型模型进行翻译的演变。

**算术任务**：做算术题（2-5位数的加法，减法，乘法）。

**文章生成任务**：给定标题和副标题，生成新闻文章。

**其他任务**：包括使用新词，纠正语法，词汇替换，多选题等等

<h3 id='20.什么是分词?'>20.什么是分词?</h3>

**分词**是将连续的文本字符串分割成有意义的词元（token）序列的过程，这可以被视为自然语言与机器语言之间的一种隐式映射或对齐方式。通过分词，文本数据被转换成机器可以理解和处理的形式。常见的分词方法有：**基于空格的分词**，**字节对编码**，**Unigram 模型** 。

**基于空格的分词**：对于英文文本来说，由于其结构特点（单词之间通常由空格分隔），使用 text.split(' ') 方法进行分词是一种简单且直接的手段。

**字节对编码**：字节对编码（BPE）算法通过训练数据学习分词器，初始将每个字符作为单独的词元，然后合并频繁共现的词元对以构建词汇表，直至达到所需的大小。

**Unigram 模型**：Unigram模型是一种基于目标函数的分词方法，它通过统计每个词汇在训练数据中的出现次数来估计其概率，并通过计算整个训练数据的似然值来评估分词结果的质量。


<h3 id='21.常见的分词原则有哪些?'>21.常见的分词原则有哪些?</h3>

**分词原则**：
- **颗粒度越大越好**。词组的字数越多，所表示的含义越具体，语义分析的结果越精准。
- **分词结果中非词典词和单字词越少越好**。**非词典词**通常意味着文本中存在一些不在词典中的新词或专有名词。因为模型没有足够的训练数据来正确预测这些词，过多的非词典词可能会导致分词错误。单字词是指由单个汉字组成的词。单字词通常不是完整的词组，它们可能只是词的一部分，或者是停用词（如“的”、“了”、“是”等）。因为它们不能表达完整的语义信息，单字词的出现可能会导致分词结果的不准确。
- **总体词数越少越好**。在相同字数的情况下，总词数越少，通常意味着每个词组包含的语义信息越多，这有助于提高分词的准确性。但这并不意味着极端的少也是好的，因为过分的切分可能会导致语义信息的丢失。


<h3 id='21.什么是最大匹配算法?'>21.什么是最大匹配算法?</h3>

**最大匹配算法**是一种基于词典的分词技术，其核心任务是将以连续字符形式存在的文本拆解成一系列有意义的词语。该算法的工作原理是从文本中提取尽可能长的词语，并与预先设定的词库进行匹配。若提取的词语在词库中存在，则将其从文本中分离出来；若不存在，则缩短一个字符后再次尝试匹配，如此循环，直至文本中的每个字符都被成功分词。根据匹配的方向不同，最大匹配算法可以分为**正向最大匹配法**、**逆向最大匹配法**以及**双向最大匹配法**。

- **正向最大匹配算法**：像吃蛋糕一样，从蛋糕的一头开始，尽可能大口地吃（匹配最长的词语）。如果这一大口（词语）在词典中能找到，就确认吃下（记录这个词语），然后继续从剩下的蛋糕（文本）开始新的一口。如果这一大口（词语）在词典中找不到，就少吃一点（减少一个字），再尝尝看。重复这个过程，直到整个蛋糕（文本）被吃完（分词完成）。

- **逆向最大匹配法**：与正向最大匹配算法相反，这次是从蛋糕的另一头开始吃（从文本的末尾开始匹配）。同样地，尽可能大口地吃，如果词典中有这个词，就确认吃下，否则少吃一点再尝。直到蛋糕被吃完为止。

- **双向最大匹配法**：这个方法就像是同时从蛋糕的两头开始吃。分别用正向和逆向的方法吃蛋糕，然后比较哪种吃法更好（哪种分词结果更合理）。最后选择一种最好的吃法（分词结果）。


<h3 id='22.如何解决模型规模过大导致的难以扩展问题？'>22.如何解决模型规模过大导致的难以扩展问题？</h3>

常见的语言模型开发主要依赖于**稠密的Transformer模型架构**，其中GPT-3等模型通过堆叠多达96层的Transformer实现了强大的语言处理能力。然而，随着模型规模的不断增大，其对计算资源的需求也在急剧上升。这种增长导致模型训练和部署必须依赖于分布式系统，将模型分布在多个GPU上。但是，这种方法已经接近了技术和硬件上的**极限**，因此，探索新的模型架构以实现更好的稀疏性成为了解决扩展难题的关键。**混合专家模型（MoE）**和**基于检索的模型**提供了有效的解决方案。

**MoE模型**：
- 由多个专家网络构成，每个网络专门负责处理输入数据的一个特定子集。通过一个门控网络来决定哪些专家网络应该被激活以处理当前的输入，这样不仅实现了模型的稀疏化，还提高了计算效率。

- 这个过程可以类比为一个由多个领域专家组成的咨询委员会，每个专家都有其独特的技能和知识。面对一个特定问题时，只有那些具备相关领域知识的专家会被选中提供意见，他们的综合观点最终形成决策。

**基于检索的模型**：

- 这种模型依赖于一个庞大的原始数据存储库。当接收到一个新的输入时，模型会在存储库中检索与之相关的信息，并基于这些检索到的信息来预测输出。

- 这个过程与我们在日常生活中遇到问题时使用搜索引擎查找相关信息，然后基于这些信息作出判断非常相似。


<h3 id='23.什么是混合专家模型？'>23.什么是混合专家模型？</h3>

**基本思想**：将输入数据通过一个门控机制分配给不同的专家，然后根据这些专家的预测以及它们各自的重要性（或权重）来生成最终的输出。

**详解**：定义 $n$ 个专家，每个专家 $(1,2,...,i)$ 都有自己的嵌入矩阵 $w_i$。每个专家有自己的权重参数 $\theta_{i}$，并基于专家特性定义每个专家函数 $h_{\theta_i}(x)$。将门控函数定义为 $n$ 个专家的概率分布 $g_i(x) = \frac {e^{(w_ix)}} {\sum_{j=1}^{n}{w_jx}}$，根据输入数据动态地选择或组合多个专家的输出。那么最终模型为 $f(x) = \sum_{1}^i \underbrace{g_i(x)}*\text{gating} \underbrace{h_{\theta_i}(x)}_\text{expert}$ 。

**注意事项**：（1）专家的混合不会节省任何计算，因为前向传播仍然需要评估每个专家，而反向传播也必须接触每个专家。因此，可以选择值排名靠前的专家更新，并将其他专家规范化为0，以节约成本。（2）只有所有专家都参与进来，混合专家才有意义，因此也需要避免只有一个专家活跃的情况。

**如何应用在语言模型**：**Sparsely-gated mixture of experts**（门控函数应用于序列，混合专家应用于每个token和隔层的Transformer block，并且只在顶层进行专家的结合），**Switch Transformer** （简化的门控函数，只激活一个专家）


<h3 id='24.怎么构建大模型领域的数据集？'>24.怎么构建大模型领域的数据集？</h3>

大语言模型的训练依赖于大量涵盖广泛领域的文本数据，如WebText数据集用于GPT-2的训练，C4语料库用于T5的训练，以及CommonCrawl用于GPT-3的训练。然而，网络数据的品质参差不齐，因此提出了数据文档的概念，其要点如下：

（1）数据集创建背景：（创建动机）了解数据集为何而建； （创建者）明确数据集的作者是谁；（资金来源）知晓数据集创建的资助情况。（2）数据集组成：（实例代表性）了解数据集中的实例代表什么；（信息完整性）检查是否存在缺失信息；（机密性）确认是否包含敏感或机密数据。（3）数据收集过程：（数据获取方式）了解实例数据的收集方法；（参与人员）明确参与数据收集的人员；（报酬情况）掌握数据收集人员的报酬方式；（道德审查）确认是否进行了道德审查。（4）预处理、清理和标记：（完成情况）了解这些工作是否已实施；（软件工具）确认是否有相应的软件支持。（5）数据集使用情况：（应用任务）了解数据集已用于哪些任务；（限制性任务）明确不适合使用该数据集的任务。（6）数据分发：（分发方式）了解数据集的分发途径；（知识产权限制）确认是否存在第三方对数据的知识产权或其他限制。（7）数据集维护：（负责人）明确谁负责维护数据集；（更新情况）了解数据集是否会进行更新。
        

<h3 id='25.Decoder-only模型训练的目标函数是什么？'>25.Decoder-only模型训练的目标函数是什么？</h3>

**Decoder-only模型**通过一个上下文嵌入函数 $\phi$ 来将序列的前 $i−1$ 个词 $x_{1:i−1}$ 映射到一个嵌入向量 $\phi(x_{1:i−1})$ ；随后应用嵌入矩阵 $E$ 和 $softmax$ 函数来得到第 $i$ 个词 $x_i$ 的概率分布 $p(x_{i}|x_{1:i})$ 。那么，Decoder-only模型训练的条件分布可以表示为 $p(x_{i}|x_{1:i})=softmax(E\phi(x_{1:i-1})_{i-1})$。

为了最大化模型在数据集上的概率，我们采用**最大似然估计**来估计此模型的参数。在这一过程中，我们计算对数似然的梯度，并根据梯度方向调整参数。设 $\theta$ 是大模型的所有参数， $D$ 是所有的训练数据， $L$ 是单个语言序列的长度， 则最终的目标函数为： $f(θ) = ∑_{x ∈ D} -\log p_θ(x) = ∑_{x ∈ D} ∑_{i=1}^{L} -\log p_θ(x_i | x_{1:i-1})$。


<h3 id='26.Encoder-only模型训练的目标函数是什么？'>26.Encoder-only模型训练的目标函数是什么？</h3>

**BERT**是一种典型的encoder-only模型，其设计目标是通过对大量文本的双向编码来获得深层次的上下文理解。其目标函数包括两个部分：（1）掩码语言模型和（2）下一句预测。

**掩码语言模型**的基本思想是通过加噪然后预测来进行训练，比如：[猫，[MASK]， 老鼠] → [猫，吃，老鼠]。该模型通过输入有噪声的序列 $x_{1:L}^{noise}$ 及其上下文嵌入，预测每个token，即 $p(x_i|x_{1:L}^{noise})=softmax(E\phi(x_{1:L}^{noise})_i)$。

**下一句预测**的目的是预测第二局是否跟随第一句，比如：[[CLS]，猫，吃，老鼠，[SEP]，[它]，[吃]，[饱了]] → 1，而 [[CLS]，猫，吃，老鼠，[SEP]，[苹果]，[红了]] → 0。注：[CLS]是驱动分别任务的起始嵌入，[SEP]用于区别两个语言序列。

BERT训练的目标函数最终为下式（RoBERTa删除了下一句预测）。其中 $D$ 是训练集， $A$ 是随机噪声函数， $I$ 表示从 $1$ 到 $L$ 的随机位置， $C$ 表示是否为跟随的下一句。

$$\sum_{x_{1:L} \in D}E_{I,x_{1:L}^{noise} \sim A(x_{1:L},I)}[\sum_{i \in I} -\log p_θ(x_i^{noise}|x_{1:i-1})]+ (-logp(c|\phi(x_{1:L})_1))$$


<h3 id='27.Encoder-decoder模型训练的目标函数是什么？'>27.Encoder-decoder模型训练的目标函数是什么？</h3>

**BART**和**T5**是典型的Encoder-decoder模型，可以实现像BERT一样对输入进行双向编码或者像GPT一样进行自回归编码。**BART**采用RoBERTa相同的编码器架构和相同的目标函数进行训练。通过掩码，乱序，删除等手段，实现了分类和生成任务。

**T5**采用denoising objective作为目标函数。该目标函数的功能是：在输入样本中，用一些唯一的特殊符号<X>, <Y>来表示原始样本中被随机masked的token，而目标样本则为被masked的token序列，用输入样本中对应位置的特殊符号<X>, <Y>分隔，最后加上一个特殊符号<Z>表示序列结束。例如：原始样本 [我，在院子里，听说，猫，吃，老鼠]，输入样本 [我，<X>，听说，<Y>，吃，老鼠]，输出样本 [是的，小明，<X>，这件事，<Z>]


<h3 id='28.优化算法怎么应用在大模型的训练中？'>28.优化算法怎么应用在大模型的训练中？</h3>

常见的优化算法：随机梯度下降算法通过初始化参数后，不断随机抽取小批量数据计算梯度并更新参数；Adam算法引入了动量和自适应步长，通过初始化参数和动量，不断随机抽取小批量数据计算梯度并更新一阶，二阶动量和参数。优化算法的优化关键在于平衡参数快速收敛与处理大模型参数量带来的高内存占用的矛盾。由于稳定性问题，学习率和一些直觉（例如，二阶方法）仍然有用，但要使大语言模型有效训练，还需要克服许多其他独特的挑战。


<h3 id='29.什么是大模型的混合精度训练？'>29.什么是大模型的混合精度训练？</h3>

在处理大规模语言模型训练时，FP16（16位浮点数）格式虽然能够显著减少内存消耗，但它限制了数值的精度，特别是对于非常小的数值，比如小于2^-24的值会直接归零。因此，为了保证训练的精度和稳定性，我们通常采用FP32（32位浮点数）来进行训练。

然而，为了平衡存储和计算效率，实践中会将**模型的权重以FP32格式存储，而在计算过程中使用FP16进行前向和反向传播**。这种方法虽然可能引入一些数值上的放大误差，但能够有效减少内存使用，同时避免了梯度消失的问题，使得内存需求大约减少了一半。这样的策略在保持模型性能的同时，优化了资源的使用。

![Agent](imgs/基础知识-29.png)

<h3 id='30.Probing方法怎么用于下游任务的迁移？'>30.Probing方法怎么用于下游任务的迁移？</h3>

**Probing技术**是一种用于**分析和理解预训练语言模型内部表示**的有效手段。它通过在预训练语言模型的**最后一层**之后附加**参数较少的线性或浅层前馈网络**，即Probing预测头，有效地分析和理解模型内部表示，以评估模型对特定任务（如输出标签）的处理能力。为了将一个包含 $L$ 个token的序列合理地映射为单个/少量的token的表示，Probing技术采用了以下两种策略：**CLS token策略**和**平均化策略**来优化预测头的映射能力。

**CLS token策略**表示在序列的开始处插入一个特殊的分类token（CLS），这个token的目的是聚合整个序列的信息。在模型的输出中，CLS token对应的表示被用作整个序列的表示。**平均化策略**假设序列中的每个token都为整个序列的语义贡献了等量的信息。因此，取序列中所有token的表示的平均值，以此来创建一个单一的、全局的序列表示。Probing方法在训练过程中保持预训练模型的权重不变，仅对新增的预测头进行训练，大幅度降低了训练成本。


<h3 id='31.PromptTuning方法怎么用于下游任务的迁移？'>31.Prompt tuning方法怎么用于下游任务的迁移？</h3>

**Prompt tuning方法**专注于优化**输入提示**，而不涉及修改语言模型的内部参数。该方法通过在原始输入前添 $k$ 个可学习的连续tokens，使得新的输入长度变为 $L= L + K$ 。这些额外的tokens的嵌入是通过在带标签的任务数据上进行训练来学习的。在整个微调过程中，**预训练的语言模型保持冻结状态**，即模型的主体参数不会发生变化。随着预训练模型规模的增大，prompt tuning的性能表现越来越出色，有时甚至能够与全面微调的效果相匹配。其中，该方法的初始化策略包括： **随机词汇嵌入**（选择随机的词嵌入），**类标签词嵌入**（选择和分类标签相关的词嵌入）和**随机初始化**（随机分配值）。

P-Tuning v2是提示调整（Prompt tuning）方法的一个改进版本，不仅仅是在输入层添加可学习的提示（prompt），而是在模型的多个层级上进行了优化。

<h3 id='32.PrefixDecoder/CausalDecoder/Encoder-Decoder的区别有哪些?'>32.PrefixDecoder/CausalDecoder/Encoder-Decoder的区别有哪些?</h3>

### Prefix Decoder：
Prefix Decoder，又被称作非因果解码器，属于Decoder only的结构体系。其输入环节采用双向注意力机制，而输出部分则运用单向注意力机制。在生成新的输出内容时，会把此前所有生成的输出都纳入考虑范围。
Prefix Decoder在对输入序列进行处理时，模型能够同时顾及序列里的全部词语。在生成输出的时候，会考量整个输入序列，而非仅仅局限于之前的输出内容。正因如此，它在应对那些需要全局上下文信息的任务时，有着更为出色的表现。在训练阶段，通常以自回归的方式展开训练，也就是说，在生成当前词语的时候，会借助之前生成的所有词语。Encoder和Decoder共同使用同一个Transformer结构，并且共享参数。
代表模型有 GLM、ChatGLM、ChatGLM2、U-PaLM 等。其适用范围是那些需要理解全文上下文，并在此基础上生成下文的任务。该结构在输入部分运用双向注意力，输出部分采用单向注意力。

### Causal Decoder：
Causal Decoder，即因果解码器，属于Decoder only的结构类型。其输入和输出均为单向注意力机制。在生成新的输出时，仅仅会考虑之前的输出，而不会涉及未来的输出。
与Prefix Decoder相比，Causal Decoder更加注重序列的时间先后关系，所以在处理时间序列数据方面具有优势。不过，在应对需要全局上下文的任务时，它的表现可能不如Prefix Decoder出色。在训练阶段，通常以自回归的方式进行训练。prefix Decoder与causal Decoder主要的差异在于注意力掩码不同。
代表模型有 GPT 系列、LLaMA-7B、BLOOM 以及 LLaMa 的衍生模型等。其适用范围是那些需要生成文本且要确保生成顺序符合因果关系的任务，比如撰写故事或者文章。不管是输入还是输出环节，都采用单向注意力机制。

### Encoder-Decoder：
Encoder-Decoder由一个编码器和一个解码器组成。编码器运用双向注意力，使得每个输入元素都能够关注到序列中的其他所有元素。解码器则采用单向注意力，保证生成的每个词仅能依赖于此前生成的词。编码器的职责是将输入数据转化为一个连续的向量，而解码器负责把这个向量转化为最终的输出结果。
Encoder-Decoder结构可以将输入数据编码为一个固定维度的向量，接着通过解码器把这个向量解码成目标输出。该结构能够有效处理长度可变的序列转换问题，并且具备较强的通用性与灵活性。在训练的时候，解码器的输入包含真实的前一个输出（遵循 teacher forcing 策略）。与 Prefix decoder 不同，这里的编码器和解码器参数是相互独立的。
代表模型有 Transformer、Flan-T5、BART 等。适用范围是那些需要理解完整输入序列并生成一个结构化输出的任务。编码器使用双向注意力，解码器使用单向注意力。


<h3 id='33.当前优化模型最主要技术手段有哪些?'>33.当前优化模型最主要技术手段有哪些?</h3>

以下是对当前优化大语言模型的主要技术手段从不同层面进行的分析：

**一、算法层面**

1. **蒸馏**：
   - **原理**：知识蒸馏是一种模型压缩技术，通过将一个复杂的、性能较好的教师模型的知识转移到一个较小的学生模型中。在大语言模型中，通常使用大型的预训练模型作为教师模型，然后训练一个较小的模型来模仿教师模型的输出。
   - **优势**：可以显著减小模型的大小和计算量，同时在一定程度上保持较高的性能。这使得大语言模型能够在资源有限的设备上运行，或者提高推理速度。例如，在移动设备上部署语言模型时，蒸馏后的小模型可以更快地响应用户请求，同时减少内存占用。
   - **应用场景**：适用于对模型大小和性能有严格要求的场景，如移动端应用、嵌入式设备等。

2. **量化**：
   - **原理**：量化是将模型的权重和激活值从高精度的数值表示（如浮点数）转换为低精度的数值表示（如整数）。通过减少数值的精度，可以降低模型的存储需求和计算量。
   - **优势**：可以大大提高模型的推理速度，减少内存占用，并且在一些情况下对模型性能的影响较小。例如，将模型从 32 位浮点数量化到 8 位整数，可以显著减少模型的大小和计算时间，同时在一些任务上可能只损失少量的准确性。
   - **应用场景**：适用于需要快速推理和低内存占用的场景，如实时应用、大规模部署等。

**二、软件层面**

1. **计算图优化**：
   - **原理**：计算图是深度学习模型在计算过程中的一种抽象表示。通过对计算图进行优化，可以减少不必要的计算、提高内存使用效率和并行性。例如，可以合并一些连续的操作，减少中间结果的存储；或者对计算图进行重排，以更好地利用硬件的并行计算能力。
   - **优势**：可以提高模型的训练和推理速度，减少资源消耗。优化后的计算图可以更高效地在各种硬件平台上运行，充分发挥硬件的性能。
   - **应用场景**：适用于各种深度学习任务，尤其是在大规模数据和复杂模型的情况下，可以显著提高训练和推理效率。

2. **模型编译**：
   - **原理**：模型编译是将深度学习模型转换为特定硬件平台上的高效执行代码。通过使用专门的编译器，可以针对不同的硬件架构进行优化，生成高效的底层代码。例如，针对 GPU 进行编译可以利用 GPU 的并行计算能力，提高模型的执行速度。
   - **优势**：可以充分发挥硬件的性能，提高模型的推理速度和效率。编译后的模型通常具有更好的内存管理和并行性，能够更好地适应不同的硬件环境。
   - **应用场景**：适用于需要在特定硬件平台上进行高效部署的场景，如数据中心、云端服务等。

**三、硬件层面**

1. **FP8（NVIDIA H 系列 GPU 开始支持 FP8，兼有 fp16 的稳定性和 int8 的速度）**：
   - **原理**：FP8 是一种低精度浮点格式，介于 FP16 和 INT8 之间。NVIDIA H 系列 GPU 对 FP8 的支持使得在进行大语言模型的计算时，可以利用 FP8 的低精度和高速度，同时保持一定的数值稳定性。
   - **优势**：相比 FP16，FP8 可以提供更高的计算速度和更低的内存占用；而与 INT8 相比，FP8 具有更好的数值稳定性，减少了精度损失的风险。这使得在大语言模型的训练和推理中，可以在不显著降低性能的情况下提高计算效率。
   - **应用场景**：适用于需要高性能计算和大规模数据处理的大语言模型任务，尤其是在使用 NVIDIA H 系列 GPU 的环境中。

综上所述，从算法、软件和硬件三个层面都有多种技术手段可以优化大语言模型。这些技术手段可以单独使用，也可以结合起来，以实现更好的性能和效率。在实际应用中，需要根据具体的任务需求、硬件资源和性能要求来选择合适的优化方法。


<h3 id='34.大模型推理加速框架有哪一些?都有什么特点?'>34.大模型推理加速框架有哪一些?都有什么特点?</h3>

1、FasterTransformer:英伟达推出的FasterTransformer不修改模型架构而是在计算加速层面优化Transformer的 encoder 和 decoder模块。具体包括如下:
a.尽可能多地融合除了GEMM 以外的操作·支持 FP16、INT8、FP8
b.移除 encoder 输入中无用的padding来减少计算开销

2、TurboTransformers:腾讯推出的TurboTransformers 由computation runtime 及 serving framework组成。加速推理框架适用于 CPU和GPU，最重要的是，它可以无需预处理便可处理变长的输入序列。具体包括如下:
a.与FasterTransformer类似，它融合了除GEMM之外的操作以减少计算量
smart batching，对于一个batch内不同长度的序列，它也最小化了zero-padding 开销·对LayerNorm和Softmax进行批处理，使它们更适合并行计算
b.引入了模型感知分配器，以确保在可变长度请求服务期间内存占用较小

<h3 id='35.大语言模型命名中7B、13B、540B是什么意思？'>35.大语言模型命名中7B、13B、540B是什么意思？</h3>
在大语言模型命名中，7B、13B、540B 通常指的是模型的参数量。

“B”是指“billion”（十亿）。

例如：
- 7B 表示该模型具有 70 亿个参数。
- 13B 即表示有 130 亿个参数。
- 540B 则意味着模型含有 5400 亿个参数。

一般来说，参数量越大的模型，通常具有更强的语言理解和生成能力，但同时也需要更多的计算资源和时间来进行训练和推理。不同规模的模型适用于不同的应用场景，开发者会根据具体需求选择合适规模的大语言模型。


<h3 id='36.为什么现在的大模型结构大部分是Decoderonly结构?'>36.为什么现在的大模型结构大部分是Decoder only结构?</h3>
现在的大语言模型结构很多采用Decoder only结构，主要有以下原因：

**一、语言生成任务适应性强**

1. 专注于生成任务：
   - Decoder only结构天然地适合语言生成任务。在这类任务中，模型的目标是根据给定的提示或上下文生成连贯的文本。例如，在文本生成、对话系统、故事续写等场景中，模型需要不断地生成新的单词或句子来回应输入。Decoder only结构能够直接从左到右依次生成输出，与人类的语言生成过程相似，更容易学习语言的模式和规律。
   - 相比之下，Encoder-Decoder结构虽然也能用于生成任务，但在一些情况下可能会因为编码器和解码器之间的交互不够直接而影响生成效果。例如，在机器翻译任务中，编码器需要将源语言句子编码成一个中间表示，然后解码器再根据这个中间表示生成目标语言句子。这种两步走的方式在处理一些复杂的语言生成任务时可能会引入额外的复杂性。

2. 自回归特性：
   - Decoder only结构通常采用自回归的方式进行训练和生成。这意味着在生成每个单词时，模型会基于之前生成的单词进行预测。这种方式能够充分利用语言的序列性和上下文信息，使得生成的文本更加连贯和自然。例如，在生成一个句子时，模型可以根据前面已经生成的单词来预测下一个最有可能出现的单词，从而逐步构建出完整的句子。
   - 自回归特性也使得Decoder only结构在处理长序列数据时具有一定的优势。由于模型是依次生成每个单词，因此可以更好地处理长文本中的长期依赖关系，避免信息的丢失。而在一些其他结构中，可能会因为处理长序列数据的困难而导致性能下降。

**二、训练效率高**

1. 并行计算：
   - 在训练过程中，Decoder only结构可以利用并行计算来提高训练效率。由于模型是从左到右依次生成输出，因此可以同时计算多个位置的输出，而不需要像Encoder-Decoder结构那样等待编码器的结果。例如，在使用大规模数据集进行训练时，可以将数据分成多个批次，每个批次中的句子可以同时进行计算，从而大大加快训练速度。
   - 此外，一些先进的训练技术，如混合精度训练、分布式训练等，也可以更容易地应用于Decoder only结构，进一步提高训练效率。

2. 数据效率：
   - Decoder only结构通常在数据效率方面表现出色。由于模型专注于生成任务，因此可以从大规模的无标注文本数据中学习语言知识。这些无标注数据通常比较容易获取，而且数量巨大，可以为模型提供丰富的语言模式和上下文信息。相比之下，Encoder-Decoder结构可能需要更多的有标注数据来进行训练，而有标注数据的获取通常比较困难和昂贵。
   - 另外，Decoder only结构还可以通过自监督学习的方式进行训练，例如使用语言建模任务（预测下一个单词）作为训练目标。这种自监督学习方式可以充分利用大量的无标注数据，提高模型的泛化能力和性能。

**三、模型灵活性和可扩展性高**

1. 易于调整和优化：
   - Decoder only结构相对简单，更容易进行调整和优化。开发者可以根据具体的任务需求和性能要求，对模型的结构、参数、训练策略等进行灵活的调整。例如，可以增加或减少模型的层数、调整注意力机制的参数、使用不同的激活函数等，以提高模型的性能和效率。
   - 此外，由于Decoder only结构的生成过程是直接从左到右依次进行的，因此可以更容易地进行在线学习和增量学习。这意味着可以在模型已经训练好的基础上，继续使用新的数据进行训练和优化，而不需要重新训练整个模型。

2. 可扩展性强：
   - Decoder only结构可以很容易地扩展到更大的规模。随着计算资源的不断增加和技术的不断进步，现在已经可以训练出具有数十亿甚至数百亿参数的大语言模型。这些大规模的模型通常采用Decoder only结构，因为它们可以更好地利用大规模数据和计算资源，提高模型的性能和泛化能力。
   - 同时，Decoder only结构也可以通过集成多个模型或使用模型并行化等技术来进一步提高性能和可扩展性。例如，可以将多个不同的Decoder only模型进行集成，或者将一个大模型分成多个小模型进行并行计算，以满足不同的应用需求。


<h3 id='37.目前各LLMs都使用哪种激活函数?'>37.目前各LLMs都使用哪种激活函数?</h3>
目前不同的大语言模型（LLMs）可能会使用以下几种常见的激活函数：
	
**一、ReLU（Rectified Linear Unit，修正线性单元）**

1. 特点：
   - 计算简单高效，只需要进行简单的比较和乘法运算。
   - 对于正输入，输出等于输入，对于负输入，输出为零，这使得它具有一定的稀疏性激活特性，有助于缓解过拟合问题。
   - 在训练过程中能够加快收敛速度，因为它不会像一些传统激活函数那样在负区间产生饱和现象。

2. 应用场景：
   - 在许多大语言模型的早期版本中广泛使用。例如在一些基础的神经网络层中，ReLU 可以有效地传递信息，促进模型的学习。
   - 对于大规模的语言模型，由于其计算效率高，能够在大规模数据和复杂模型结构下快速处理信息。

**二、GELU（Gaussian Error Linear Unit，高斯误差线性单元）**

1. 特点：
   - 是一种平滑的激活函数，它的输出在输入值较小时会有一定的平滑过渡，而不是像 ReLU 那样突然截断。
   - 基于输入的高斯分布进行计算，具有一定的随机性和不确定性，这有助于模型更好地捕捉数据的多样性和复杂性。
   - 在训练过程中表现出较好的稳定性和收敛性，能够提高模型的性能和泛化能力。

2. 应用场景：
   - 在一些先进的大语言模型中得到广泛应用。例如在 Transformer 架构的语言模型中，GELU 可以为注意力机制和前馈神经网络层提供更平滑的激活效果，从而提高模型对语言的理解和生成能力。
   - 对于需要处理复杂语义和上下文信息的任务，GELU 能够更好地适应不同的输入情况，生成更准确和自然的语言输出。

**三、Swish**

1. 特点：
   - 具有非线性和光滑的特性，能够在不同的输入范围内提供连续的激活输出。
   - 类似于 ReLU 和 Sigmoid 函数的组合，在正区间具有类似于 ReLU 的线性增长特性，在负区间又有一定的平滑过渡，避免了 ReLU 的硬截断问题。
   - 在训练过程中能够自适应地调整激活强度，根据输入的大小动态地调整输出的幅度，有助于提高模型的表达能力。

2. 应用场景：
   - 在一些追求高性能的大语言模型中可能会被使用。例如在对语言的细微差别和复杂关系要求较高的任务中，Swish 可以为模型提供更丰富的非线性表达能力，从而提高模型的准确性和灵活性。
   - 对于需要快速适应不同数据分布和任务要求的模型，Swish 的自适应特性可以使其在不同的场景下都能发挥较好的作用。

**四、SwiGLU**

SwiGLU（SwiGLU activation function）是一种在大语言模型中可能会被使用的激活函数。

1. 特点：
（1）结合了 Swish 和 Gated Linear Unit（GLU）的特性：
   - Swish 函数具有平滑的非线性特性，能够在不同的输入范围内提供连续的激活输出，有助于模型更好地捕捉复杂的非线性关系。
   - GLU 通过门控机制对输入进行筛选和控制，能够增强模型的表现力和对重要信息的关注。SwiGLU 将两者结合起来，综合了它们的优点。
（2）自适应和动态性：
   - 可以根据输入的变化自适应地调整激活强度和门控状态，动态地适应不同的任务和数据分布。这使得模型能够更加灵活地处理各种输入情况，提高性能和泛化能力。
（3）高效计算：
   - 通常在计算上相对高效，不会引入过多的计算复杂度。这对于大规模的语言模型来说非常重要，可以在保持高性能的同时，减少训练和推理的时间成本。

2. 应用场景：

（1）大语言模型中的中间层和输出层：
   - 在语言模型的神经网络结构中，可以应用于中间隐藏层，帮助模型更好地学习语言的特征和模式。同时，在输出层也可以使用 SwiGLU 来生成更准确和自然的语言输出。
（2）复杂语言任务：
   - 对于需要处理复杂语义、上下文理解和生成高质量文本的任务，如文本生成、机器翻译、问答系统等，SwiGLU 激活函数可以提供更强大的非线性表达能力和适应性，提高模型的性能。
（3）大规模训练和微调：
   - 在大规模数据上进行训练时，SwiGLU 可以帮助模型更好地收敛和泛化。同时，在对预训练模型进行微调时，也可以根据具体任务的需求调整 SwiGLU 的参数，以获得更好的效果。


<h3 id='38.介绍一下FFN块计算公式'>38.介绍一下 FFN 块 计算公式</h3>

前馈神经网络FFN: 用于处理自注意力机制中编码的信息，是非线性层。由两个线性转换层组成，中间有一个非线性激活函数。

1. 输入层到隐藏层： Z = W1 · x + b1
   
2. Z是隐藏层的线性变换输出, W是权重矩阵, x是输入变量, b是偏置项
   
3. 激活函数:  f(Z) = f(W1 · X + b1)     f()表示激活函数

4. 隐藏层到输出层：  FFN(x) = W2 · f(Z) + b2 = W2 · f(W1x+b1) + b2
   

其中W2 是输出层的权重矩阵, b2 是输出层的偏置项。


<h3 id='39.进行SFT操作的时候，基座模型选用Chat还是Base?'>39.进行SFT操作的时候，基座模型选用Chat还是Base?</h3>

在进行监督微调（SFT）时，选择基座模型是一个重要的决策。一般来说，应该根据你的具体应用需求和数据类型来选择“Chat”还是“Base”模型：

1. **选择 Base 模型的情况**：
   - 如果你的 SFT 数据是通用的、广泛的，且不需要任何对话特定的能力，例如一些开放领域的问答或内容生成任务，那么使用 “Base” 模型通常是更合适的。
   - Base 模型未经过特定的对话式微调，因此适合用作一个通用的预训练模型，可以更容易地迁移到特定领域上，而不需要担心已有的对话式结构影响。

2. **选择 Chat 模型的情况**：
   - 如果你的 SFT 数据涉及对话任务，或者需要支持交互式的用户体验，那么 Chat 模型可能是更好的选择。
   - Chat 模型通常已经接受过对话式的预训练，具备一定的上下文记忆能力、对话结构，以及一些通用的礼貌和格式化规则。如果你的SFT任务是为了进一步优化这些对话能力或者是定制化一些特定的对话风格，基于Chat模型会让你少做很多低层的微调工作。

**简而言之**：
- **Base 模型**适合领域特定的数据或内容生成任务，适合构建从头开始的应用。
- **Chat 模型**适合对话、交互式任务或以对话为核心的应用。

希望这能帮助你更好地选择适合的基座模型！

<h3 id='40.如果想要在某个模型基础上做全参数微调，需要多少显存?如何计算？'>40.如果想要在某个模型基础上做全参数微调，需要多少显存?如何计算？</h3>

在做全参数微调时，所需的显存量取决于多个因素，包括模型的大小、批量大小、优化器类型、显存优化技术（如梯度检查点和混合精度）等。以下是具体的计算方法和示例，以帮助估算显存需求。

### 1. 显存需求的基本组成
显存的主要消耗来自以下几个部分：
   - **模型参数**：存储模型的权重。
   - **优化器状态**：包括梯度和动量等。不同的优化器需要的显存量不同。
   - **前向传播激活值**：用于反向传播计算梯度。
   - **微调过程的临时存储**：如梯度缓存、梯度检查点等。

### 2. 参数显存估算公式
假设模型的参数数量为 \( P \)，我们可以通过以下公式估算显存消耗：

- **模型参数**：需要 \( 4 \times P \) 字节（32位浮点数）。
- **优化器状态**：通常是参数大小的 2 倍（Adam 优化器较常见，存储动量和二阶动量），即 \( 8 \times P \) 字节。
- **激活值存储**：根据批量大小（batch size, B）和输入长度（例如序列长度，L）来估计，每层大约存储一个前向激活值。通常我们假设每个激活值占用与参数类似的内存。

因此，显存需求大致为：
\[
\text{显存需求} \approx 4 \times P + 8 \times P + \text{激活值所需显存}
\]

### 3. 示例：不同规模模型的显存需求
假设我们有以下模型，使用单精度（32位浮点数）：

- **GPT-2 Small**（1.5亿参数）：约 600 MB（模型参数） + 1.2 GB（优化器） + 激活值
- **GPT-3 Small**（1.75亿参数）：约 700 MB（模型参数） + 1.4 GB（优化器） + 激活值
- **GPT-3 Large**（13亿参数）：约 5.2 GB（模型参数） + 10.4 GB（优化器） + 激活值
- **GPT-3 175B**：约 700 GB（模型参数） + 1.4 TB（优化器） + 激活值

### 4. 计算激活值显存
激活值的显存需求主要取决于批量大小和序列长度。假设每个激活值占用与参数相当的内存，每层都会存储激活值。如果模型有 \( L \) 层、批量大小为 \( B \)，序列长度为 \( T \)，每个激活值大约占 \( 4 \times T \times B \) 字节。

对于较大的模型（如 175B 参数的 GPT-3），激活值消耗甚至可能超过模型参数本身。为了控制显存使用，可以使用 **梯度检查点** 和 **混合精度训练** 等技术。

### 5. 如何降低显存需求
以下是一些减少显存占用的技巧：
- **混合精度训练**：将模型参数从 32 位浮点数改为 16 位浮点数，节省约一半显存。
- **梯度检查点**：在前向传播中保存较少的激活值，减少反向传播所需的显存。
- **分布式训练**：使用多 GPU 来分担显存压力，例如通过模型并行和数据并行。

### 6. 实际估算显存需求
举个简单例子，如果要全参数微调一个 10 亿参数（1B）的模型，使用 Adam 优化器和批量大小 4，序列长度为 1024：
- **模型参数显存**：约 4 GB
- **优化器状态**：约 8 GB
- **激活值（假设 24 层 Transformer）**：约 4 GB

总计大约需要 16 GB 显存，可能还需要额外的缓存。

<h3 id='41.为什么SFT之后感觉LLM傻了?'>41.为什么SFT之后感觉LLM傻了?</h3>

在对大语言模型（LLM）进行监督微调（SFT）后，模型可能出现“变傻”的现象，这是一些常见问题引起的。以下是可能的原因和相应的解释：

### 1. **训练数据的单一性或质量问题**
   - **数据过于单一**：如果用于微调的数据太局限，模型会在微调过程中“忘记”一些预训练期间学到的通用知识，变得更加“狭隘”。例如，如果只用特定格式的问答数据进行微调，模型的回答可能会受限于这种格式。
   - **数据质量差**：如果 SFT 数据中包含错误、不连贯或不准确的信息，模型会在微调时吸收这些低质量信息，从而导致“变傻”。确保数据准确、符合实际应用场景非常重要。

### 2. **过度拟合或灾难性遗忘**
   - **过度拟合**：如果训练次数过多或学习率过高，模型可能会过度拟合到微调数据上，从而损失了泛化能力。模型会变得“偏执”，只会回答和微调数据非常类似的问题。
   - **灾难性遗忘**：模型可能会在微调过程中丧失之前学到的一些知识，尤其是当微调数据相对较少时。这导致模型在回答通用问题时表现变差。

### 3. **监督信号过强，损失灵活性**
   - SFT通常使用强监督信号来让模型学会特定格式或风格，导致模型生成的回答更加固定化、缺乏灵活性。例如，模型在生成回答时会倾向于采用模板化的回答方式，而忽略多样化表达。
   - 为了达到符合特定格式的目标，SFT过程可能会抑制模型原有的推理能力，使其变得“死板”。

### 4. **对话模式的影响**
   - 如果 SFT 数据包含大量特定对话模式或礼貌用语，模型可能会变得更加“迎合”用户，而不是做出准确的回答。模型可能会过度强调顺从性、礼貌性，忽略原有的逻辑性和准确性。

### 5. **优化目标的局限**
   - 在 SFT 中，模型的训练目标可能过于关注特定的指标（如准确率、格式化等），忽略了实际对话和理解中的灵活性和创造性。例如，如果优化目标过于关注简洁回答，模型可能会忽略额外信息，变得“简短而浅显”。
   - 损失函数的选择不当也可能导致模型在某些回答场景中表现变差。例如，如果损失函数过于简单或缺乏多样性，模型会对某些问题给出单调的回答。

### 6. **微调时未使用足够的正则化**
   - 缺乏正则化（如Dropout、权重惩罚等）可能会导致模型在微调过程中迅速适应特定的 SFT 数据，但失去泛化能力，使模型更容易出现灾难性遗忘或过度拟合。

### 7. **参数调整不当**
   - 超参数（如学习率、批量大小、优化器等）的选择对微调效果影响很大。较高的学习率可能导致模型忘掉原有知识，而太低的学习率可能会使 SFT 效果不明显，达不到期望目标。

### 如何改进 SFT 过程以避免“变傻”
1. **使用多样化和高质量的数据**：确保 SFT 数据覆盖丰富的场景，避免过度局限于某种特定格式或领域。
2. **加入原始数据进行正则化训练**：在微调过程中，可以加入部分预训练数据或通用对话数据，以帮助模型保留其原有知识。
3. **调整超参数和正则化策略**：使用适当的学习率和训练轮数，避免模型过度拟合到 SFT 数据。
4. **使用适当的损失函数**：选择合适的损失函数，避免对特定格式或风格过度优化。
5. **使用增量训练或混合精度训练**：可以在不同的数据集上逐步微调，使模型保留更多的原始知识。

通过以上改进措施，可以在 SFT 的过程中更好地平衡模型的“专精”与“通用”能力，避免模型“变傻”。


<h3 id='42.领域模型Continue PreTrain 数据如何选取?'>42.领域模型Continue PreTrain 数据如何选取?</h3>

在构建领域模型并继续预训练（Continue Pretrain）时，数据的选择和质量对最终效果至关重要。领域数据应该能够有效地丰富模型在该领域的知识和语言特征，同时避免信息噪声。以下是选取领域数据的一些关键原则和方法：

### 1. **确保领域相关性**
   - 选择与目标领域高度相关的文本，确保数据覆盖目标领域的关键术语、概念和语言风格。例如：
     - 医疗领域的模型：应使用医学文献、临床报告、健康网站的内容。
     - 金融领域的模型：应包含金融新闻、财经报告、行业研究、市场分析。
     - 法律领域的模型：需要法律条文、判决书、合同范本等内容。
   - **来源选择**：可以从专业文献、行业报告、领域相关的论坛和讨论网站、官方文件等来源收集数据，以保证数据的专业性和准确性。

### 2. **确保数据质量**
   - **去除非结构化和低质量文本**：剔除拼写错误、语法混乱、不完整的句子和冗余内容。可以使用自动化清洗工具，或者手动筛选一些样本，确保质量。
   - **去除重复数据**：大量重复的数据可能会导致模型的过拟合，因此需要去重。
   - **数据标注**：在某些领域中，经过标注的数据更有帮助。例如在法律领域，标注不同类型的法律文件或术语；在医学领域，标注不同的病症、药物等信息。这可以帮助模型更好地理解数据结构。

### 3. **数据覆盖广度**
   - 选择包含多种类型的领域文本，以确保模型具有良好的泛化能力。例如，在金融领域，可以选择涵盖股票、银行、宏观经济、政策分析等内容的数据集。
   - **时效性**：一些领域（如科技、金融）可能对时效性有要求，选择较新的数据有助于模型掌握最新的术语和趋势。

### 4. **控制数据分布和比重**
   - **控制领域数据与通用数据的比例**：领域数据在整体数据中的占比需要合适，以达到细化模型知识的效果。如果只使用单一领域数据，模型可能丧失其通用性。可以通过逐步增加领域数据的比例来控制知识的迁移速度。
   - **分布平衡**：在领域数据内部，也需要确保不同类型数据的均衡。例如，在医学领域，可以均衡医学文献、病历记录、药物说明等的数量。

### 5. **关注领域中的特定语言特征**
   - **术语和习惯用语**：确保数据集中包含领域内常见的术语、缩写和习惯用语，例如在金融领域的“IPO”、“流动性”、在医学领域的“抗体”、“高血压”等。
   - **风格和语气**：某些领域（如法律、学术）通常有特定的写作风格和语气，选择数据时应尽量保持一致，这有助于模型生成符合领域特征的文本。

### 6. **数据来源建议**
   - **公开数据集**：可以优先考虑一些经过整理的领域数据集（如 PubMed 医学数据集、法律判例集等），这些数据集通常质量较高、结构较清晰。
   - **行业文档**：从相关领域的专业报告、白皮书、技术手册、行业期刊等提取内容。
   - **领域内的社区和论坛**：在一些专业领域中，论坛和问答社区（如Stack Overflow、医学论坛）是获取实用领域数据的好来源。
   - **网络爬取**：可以从专业网站、领域相关博客等爬取数据，需注意数据版权和清洗。

### 7. **混合预训练策略**
   - 为了保留模型的通用能力，可以采用混合预训练的策略。比如将通用数据与领域数据交替进行训练，或分阶段进行训练：
     - **阶段式训练**：首先在通用数据上训练一段时间，然后逐步切换到领域数据上。
     - **交替训练**：在每个训练 epoch 中交替使用领域数据和通用数据，以防止模型“遗忘”通用知识。

### 8. **引入数据增强和多样化**
   - **数据增强**：通过同义词替换、句式变换等方法增加数据的多样性，有助于模型更好地泛化。
   - **合成数据**：在某些稀缺领域（如特定医学病历），可以生成一些合成数据或使用其他类似的领域数据以增强数据集。

### 总结
在选取领域数据进行继续预训练时，核心是找到高质量、广泛覆盖且适当平衡的数据。领域数据要具备专业性和时效性，同时需要考虑与通用数据的比重，以确保模型在特定领域内知识更丰富的同时，仍然保持一定的通用能力。


<h3 id='43.领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力?'>43.领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力?</h3>

在领域数据上训练模型后，常会出现“灾难性遗忘”的现象，即模型在特定领域表现提升的同时，通用能力有所下降。这种问题在自然语言处理模型中尤为显著，因为领域数据会覆盖一些常用的通用数据，导致模型过度专注于该领域。要缓解模型遗忘通用能力，可以采取以下几种策略：

### 1. **混合数据训练**
   - **通用数据与领域数据混合训练**：在继续预训练（Continue Pretrain）时，将通用数据和领域数据按比例混合。例如，可以设置领域数据占 70%，通用数据占 30%，以平衡模型的领域知识和通用能力。
   - **动态数据比例调整**：在训练的初期可以多用领域数据，逐渐增加通用数据的比例。这种做法可以帮助模型先掌握领域知识，再逐步恢复和加强通用能力。

### 2. **交替训练（Interleaved Training）**
   - 在每个 epoch 或训练循环中交替使用领域数据和通用数据进行训练。例如，一个 epoch 使用领域数据，接下来一个 epoch 使用通用数据。这样可以让模型在每次接触领域数据后“复习”通用数据，减少遗忘。
   - **多任务训练**：通过多任务训练的方式，可以同时训练模型的通用任务和领域任务，让模型学习在不同任务中保持平衡。这种方式尤其适合有特定任务需求的模型，如问答、文本生成等。

### 3. **少量微调（Lightweight Fine-Tuning）**
   - **冻结部分通用层**：在微调过程中，冻结模型的前几层或部分网络层，使其保持原有的通用知识。这样可以只更新模型的高层次表示层或最后几层，以增强模型对领域数据的适应性，同时尽量减少对通用能力的影响。
   - **适当降低学习率**：使用较小的学习率进行微调，可以减少模型在通用数据上的遗忘速度。这有助于平衡模型对通用知识和领域知识的掌握。

### 4. **知识蒸馏（Knowledge Distillation）**
   - 使用蒸馏技术，将通用模型的知识“蒸馏”到新模型中。例如，在训练领域模型时，以预训练模型（教师模型）为参考，让微调后的模型（学生模型）模仿教师模型的输出。这种方法可以在微调过程中保持模型的通用能力。
   - 蒸馏过程中，可以设置损失函数，使学生模型在学习领域知识的同时，仍然尽量保持与教师模型的输出一致性，从而保留通用知识。

### 5. **正则化策略**
   - **Elastic Weight Consolidation (EWC)**：EWC是一种常见的防遗忘策略，通过在训练时对模型参数施加正则化约束，确保模型不会过度偏离原来的通用知识。它根据参数对模型性能的影响程度来设置不同的约束权重，保护重要的参数。
   - **L2正则化**：通过引入 L2 正则化，可以在训练过程中限制模型的参数更新幅度，保持与预训练模型的权重距离，减少对通用知识的遗忘。

### 6. **知识回放（Replay）**
   - 定期将通用数据作为“回放”数据引入模型训练过程。例如，训练一段时间领域数据后，再使用通用数据对模型进行短暂训练。这类似于让模型“复习”之前的知识，可以缓解遗忘问题。
   - 在使用回放策略时，可以随机选择通用数据中的一小部分，以降低训练成本，同时在模型内保持通用知识的活性。

### 7. **层次式微调（Layer-Wise Tuning）**
   - 通过逐层微调的方式，让模型逐步适应领域数据。例如，先微调高层（接近输出端）的几层，然后逐步向低层（接近输入端）微调。这样可以使模型在保留通用知识的前提下，逐步调整到领域需求上。
   - 这种方法的优点在于，模型的低层通常包含更多通用信息，而高层则倾向于捕捉特定任务的特征。逐层微调可以帮助模型在训练过程中逐渐适应领域知识。

### 8. **基于参数高效的适配（Parameter-Efficient Adaptation）**
   - 使用参数高效的适配方法，如 **Adapter Layer** 或 **LoRA（Low-Rank Adaptation）**，在模型的特定层插入适配模块，专门用于学习领域数据，而不对模型的主要参数进行大幅调整。
   - 这种方法通过在不改变模型核心参数的情况下引入新的知识，从而保留原有的通用能力，同时实现对领域的适应性。

### 9. **保持评估与调试**
   - 在训练过程中，定期评估模型的通用任务性能，例如在常用的通用数据集（如Wiki数据、通用问答集）上进行测试。如果发现模型在通用任务上的性能下降过快，可以采取调整策略，如增加通用数据训练或降低学习率。
   - 通过评估模型在领域和通用任务上的平衡情况，及时调整训练策略，以确保模型在微调后不会完全偏向特定领域。

### 总结
为减缓模型遗忘通用能力，可以通过混合数据、交替训练、正则化、知识蒸馏等多种方法，使模型在学习领域知识的同时保留通用知识。不同方法适合不同的场景，可以根据具体需求结合使用，从而在领域知识与通用能力之间找到平衡。



<h3 id='44.什么是分布式训练？'>44.什么是分布式训练？</h3>

**定义**：分布式训练是指将机器学习或者深度学习模型训练任务分解为多个子任务，并在多个计算设备上并行训练的过程。该过程需要多个设备进行计算，还涉及设备之间的数据传输。

**优势**：可以加速模型的训练。

**常见策略**：数据并行，模型并行。

![](./imgs/并行策略.png)

假设神经网络中某一层是做矩阵乘法，输入为 4×5 的矩阵 $x$ ，模型参数为 5×8的矩阵 $w$ ，那么该层的输出为4×8的矩阵 $out$。

单机单卡的训练，该层先计算得到 $out$，并将 $out$ 传递给下一层，最终得到 $loss$然后再反向传播过程中，通过 $\frac {\partial {loss}} {\partial w}$ 更新 $w$。而分布式训练则是通过切分 $x$ 或者 $w$ 进行训练加速。其中，根据切分的是 $x$ 还是 $w$ ，分为”数据并行“和”模型并行“。



<h3 id='45.大模型为什么需要分布式训练？'>45.大模型为什么需要分布式训练？</h3>

由于内存墙的存在，单一设备的算力及容量，受限于物理定律（电磁学，光学，量子力学等），导致持续提高芯片的集成越来越困难，难以满足大模型的训练需求。

**内存墙**：计算机处理器和内存之间速度差异的瓶颈问题。即CPU的处理速度远远超过了内存的访问速度，导致CPU在处理任务时常常需要等待内存的响应，从而限制了计算机整体性能的提升。



<h3 id='46.什么是数据并行策略？'>46.什么是数据并行策略？</h3>

**定义**：数据并行就是将数据 $x$ 进行切分，而每个设备上的模型参数 $w$ 相同。

**并行流程**：

![](./imgs/数据并行.png)

在分布式训练中，当我们把输入数据 $x$ 沿第0维度平均分配到两个计算设备上时，每个设备上得到的输出将只是最终逻辑输出的一半。这意味着，要获得完整的逻辑输出，我们需要将两个设备上的输出结果进行拼接。

然而，由于数据被分配到不同的设备上，在反向传播计算损失函数对权重 $w$ 的梯度时，每个设备上计算得到的 $\frac {\partial {loss}} {\partial w}$ 是不同的。这种差异会导致各设备上的模型训练状态不一致，进而影响模型的收敛性和性能。

为了解决这个问题，在反向传播过程中，我们必须对所有设备上的梯度进行**AllReduce**操作。AllReduce操作能够确保各个设备上的梯度信息进行汇总和平均，使得每个设备上的模型权重更新保持一致，从而保证整个分布式训练过程中模型的一致性和准确性。

**适用范围**：当数据集较大，模型较小时，由于反向过程中为同步梯度产生的通信代价较小，此时选择数据并行一般比较有优势。



<h3 id='47.什么是数据并行策略中的AllReduce操作？'>47.什么是数据并行策略中的AllReduce操作？</h3>

**简介**：Allreduce操作是分布式训练中用于节点间同步参数的高效通信方法，确保深度学习训练的一致性并降低通信成本。

**基本原理**：先将所有节点的参数相加，然后再将结果广播到所有节点。

**实现流程**：

（1）在每个训练迭代中，每个节点使用自己的数据子集对模型进行训练。

（2）在每次迭代或一定数量的迭代后，每个节点将对自己模型参数的更新量（梯度）进行求和。

（3）每个节点将它的局部总和发送给一个指定的根节点。

（4）根节点收到所有其他节点的局部总和后，会将这些总和相加，得到一个全局的总和.

（5）根节点会将计算出的全局总和发送给所有其他节点。

（6）每个节点用接收到的全局总和减去自己原来的局部总和，得到新的参数值。



<h3 id='48.什么是模型并行策略？'>48.什么是模型并行策略？</h3>

当神经网络规模极为庞大时，数据并行方式下同步梯度的开销将显著增加，甚至可能出现网络规模超出了单个节点的存储能力。针对这一挑战，采用模型并行策略。

**定义**：模型并行一般指层内并行，就是将模型参数$x$ 进行切分，而每个设备上的数据 $w$ 相同。

**层内并行**：在同一个网络层内，参数可以被分割到不同的设备上，每个设备计算该层的一部分输出。在这种情况下，各设备上的计算通常是并行进行的，，然后汇总结果。

**并行流程**：

在分布式训练中，沿着第1维度将单层内的模型参数 $w$ 平均分配到两个计算设备上，每个设备仅负责存储和处理模型参数的一个子集。只有当所有计算设备上的这些子集模型参数拼接在一起时，才能构成完整的模型。

![](./imgs/模型并行.png)

由于每个设备在执行前向传播和反向传播时都需要对整个数据集进行操作，因此在实施模型并行策略的过程中，输入数据必须在所有计算设备之间进行广播，确保每个设备都能获得完整的输入数据集以独立处理其分配到的模型部分。



<h3 id='49.什么是流水并行策略？'>49.什么是流水并行策略？</h3>

当神经网络过于巨大，无法在单个设备上存放时，还可以采用流水并行的方式。

**定义**：流水并行一般指层间并行，就是将模型的所有网络层分块并被放置在不同的设备上。每一个设备的输出将成为下一个设备的输入。各个设备之见采用“接力”的方式完成训练。

**并行流程**：

在分布式训练中，将模型分为4个模块（ $T1$ ~ $T4$ ）, 然后均分在两个设备（GPU0和GPU1）上进行运算。GPU0上完成前两层的计算后，它的输出被当作GPU1的输入，继续进行后两层的计算。

![](./imgs/流水并行.png)



<h3 id='50.什么是混合并行策略？'>50.什么是混合并行策略？</h3>

在网络的训练过程中，可以综合运用数据并行、模型并行以及流水并行等策略。以GPT-3为例：

![](./imgs/混合并行.png)

（1）将整个模型被划分为64个阶段，实现整体模型的流水并行策略；

（2）在每个阶段，采用6台DGX-A100主机，实现单阶段的数据并行策略；

（3）在每台主机上，采用8张GPU显卡实现单台机的模型并行策略。



<h3 id='51.什么是大模型的有害性（危害）？'>51.什么是大模型的有害性（危害）？</h3>

大模型的有害性包括：**性能差异**，**社会偏见**，**有害信息和虚假信息**

**性能差异**：性能差异意味着大模型对于特定任务，在某些群体中表现更好，在其他群体中表现更差。例如，自动语音识别模型对黑人说话者的识别性能要差于白人说话者。

**社会偏见**：社会偏见是将某个概念与某些群体相对其他群体进行系统关联。例如，图片生成模型生成的黑人形象相比白人更多地被描绘成贫穷和胆怯。

**有毒信息和虚假信息**：大型语言模型可能产生攻击性的、有害的内容，或者产生误导性的内容。例如：仇恨言论、骚扰、色情、暴力、欺诈、假信息和侵犯版权等。


<h3 id='52.大模型中的优化算法有哪些常见的选择？它们各有什么优缺点？'>52.大模型中的优化算法有哪些常见的选择？它们各有什么优缺点？</h3>
大模型训练中常用的优化算法包括梯度下降（SGD）、Adam、RMSProp等。SGD简单直观，但收敛速度可能较慢；Adam结合了梯度的一阶和二阶矩估计，通常具有较好的收敛速度和性能，但可能需要对学习率进行精细调整；RMSProp则是对SGD的一种改进，通过调整每个参数的学习率来加速收敛。选择哪种优化算法取决于具体任务和数据特点。

以下是大模型中常见的优化算法及其优缺点：
1. **梯度下降法（Gradient Descent）**：
    - **优点**：
      - **理论基础扎实**：是优化神经网络的基础方法，具有明确的数学原理和理论支持，易于理解和实现。对于各种可导的目标函数，都可以使用梯度下降法来寻找最优解。例如，在简单的线性回归和逻辑回归问题中，梯度下降法能够有效地找到使损失函数最小化的参数值。
      - **适用性广泛**：适用于多种类型的模型和任务，无论是简单的线性模型还是复杂的深度神经网络，都可以使用梯度下降法进行优化。它可以处理大规模数据集，通过不断迭代更新参数，使模型逐渐逼近最优解。
    - **缺点**：
      - **收敛速度慢**：尤其是在处理复杂的高维问题时，由于每次迭代只沿着梯度的反方向前进一小步，所以收敛速度可能非常缓慢。当模型参数数量众多时，需要进行大量的迭代才能达到较好的效果，这会消耗大量的计算资源和时间。
      - **易陷入局部最优**：在非凸函数的情况下，梯度下降法容易陷入局部最小值，而无法找到全局最优解。这是因为它只根据当前位置的梯度信息来决定下一步的搜索方向，对于复杂的函数地形，可能会被局部的极小值点所吸引，从而导致最终的解不是全局最优的。
      - **学习率选择困难**：学习率是梯度下降法中的一个重要超参数，它决定了每次迭代中参数更新的步长。如果学习率选择过大，可能会导致算法不收敛，跳过最优解；如果学习率选择过小，又会使算法的收敛速度非常缓慢，需要更多的迭代次数才能达到较好的效果。
2. **随机梯度下降（Stochastic Gradient Descent，SGD）**：
    - **优点**：
      - **计算效率高**：每次迭代只使用一个样本来更新模型参数，相比于批量梯度下降法（需要使用整个数据集进行一次参数更新），计算量大大减少，因此在处理大规模数据集时速度更快，可以更快地得到模型的初步结果。
      - **适用于在线学习**：支持在线更新，当新样本不断涌现时，可以快速学习到新的信息并且更新模型。这对于实时性要求较高的应用场景，如在线推荐系统、实时预测等非常重要。
      - **对非凸函数有一定优势**：在处理非凸函数时，由于每次只使用一个样本更新参数，更容易跳出局部极小值，有可能找到更好的局部最优解，从而在一定程度上缓解了梯度下降法容易陷入局部最优的问题。
    - **缺点**：
      - **梯度噪声大**：由于每次只考虑单个样本，计算出的梯度可能存在较大的噪声，不能很好地反映真实的梯度方向。这会导致参数更新的方向不稳定，使得损失函数在训练过程中可能会出现震荡，难以快速收敛到一个稳定的最小值。
      - **需要频繁调整学习率**：学习率的选择仍然是一个关键问题，并且由于梯度的噪声较大，学习率的调整策略需要更加谨慎。如果学习率衰减过快，可能会导致模型在还没有找到较好的解之前就停止更新；如果学习率衰减过慢，模型可能会在局部最优解附近震荡，无法进一步收敛。
      - **对离群值敏感**：单个样本中的离群值可能会对参数的更新产生较大的影响，从而影响模型的训练效果。因为每次迭代只依据一个样本的信息，所以离群值的影响在随机梯度下降中会被放大。
3. **小批量梯度下降（Mini-batch Gradient Descent）**：
    - **优点**：
      - **平衡计算效率和梯度准确性**：综合了批量梯度下降和随机梯度下降的优点，每次迭代使用一小批样本进行参数更新。这样既可以减少计算量，提高计算效率，又能够在一定程度上降低梯度的噪声，使参数的更新更加稳定，相比于随机梯度下降更容易收敛到一个较优的解。
      - **内存利用高效**：在处理大规模数据集时，不需要一次性将整个数据集加载到内存中，可以分批次读取数据进行训练，有效地利用了内存资源，降低了对硬件设备的内存要求。
    - **缺点**：
      - **超参数选择复杂**：小批量的大小是一个超参数，需要根据具体的问题和数据集进行选择。不同的问题和数据集可能需要不同的小批量大小，才能达到最佳的训练效果。选择合适的小批量大小需要进行大量的实验和调参，增加了算法的复杂性。
      - **并行计算可能受限**：在分布式计算环境中，小批量的划分和数据的分配可能会影响并行计算的效率。如果小批量的划分不均匀或者数据分配不合理，可能会导致某些计算节点的负载过重，而其他节点处于空闲状态，从而影响整个训练过程的速度和效率。
4. **动量法（Momentum）**：
    - **优点**：
      - **加速收敛**：引入了动量项，累积了之前迭代时的梯度值，使得参数的更新方向不仅仅取决于当前的梯度，还受到之前梯度的影响。在梯度方向变化较小时，动量项可以加速参数的更新，提高收敛速度；在梯度方向变化较大时，动量项可以平滑梯度的变化，减少震荡，使模型更容易收敛。
      - **增强稳定性**：对于一些存在噪声或波动的数据集，动量法可以降低梯度的敏感性，使模型更加稳定。它能够在一定程度上克服随机梯度下降中由于单个样本的噪声而导致的参数更新不稳定的问题，提高模型的鲁棒性。
    - **缺点**：
      - **增加了内存消耗**：需要保存之前迭代的梯度信息来计算动量项，这会增加一定的内存消耗。对于参数数量非常庞大的大模型来说，额外的内存消耗可能会成为一个问题。
      - **超参数调整**：动量衰减参数是动量法中的一个重要超参数，它决定了之前梯度信息对当前参数更新的影响程度。选择合适的动量衰减参数需要进行实验和调参，不同的问题和数据集可能需要不同的参数值，这增加了算法的复杂性。
5. **Adagrad 算法**：
    - **优点**：
      - **自适应学习率**：能够自动调整每个参数的学习率，根据之前的梯度信息，对于频繁更新的参数，学习率会逐渐减小，对于偶尔更新的参数，学习率会相对较大。这样可以更好地适应不同参数的更新需求，提高算法的效率和准确性，尤其对于稀疏特征的数据，能够有效地处理。
      - **无需手动调参**：在一定程度上减少了手动调整学习率的工作量，因为学习率会根据算法的运行自动调整，不需要人为地根据训练情况不断地尝试不同的学习率值。
    - **缺点**：
      - **学习率下降过快**：学习率在每次迭代中都会减小，可能会在训练过程的后期变得非常小，导致模型在训练后期出现收敛速度缓慢的问题，甚至可能无法收敛到最优解。
      - **缺乏灵活性**：对于不同的任务和数据集，Adagrad 算法对学习率的调整方式是固定的，不能根据具体情况自动调整。这意味着在某些情况下，它可能无法很好地适应模型的学习需求，导致性能不如其他优化算法。
6. **RMSProp 算法**：
    - **优点**：
      - **改进的学习率调整**：通过引入一个衰减系数，让梯度的累积量每回合都衰减一定比例，避免了 Adagrad 算法中学习率下降过快的问题。这样可以使模型在训练过程中保持较为稳定的学习率，提高了收敛速度，尤其是在处理非平稳目标时效果较好，对于循环神经网络（RNN）等模型具有较好的适用性。
      - **对梯度噪声的适应性**：能够较好地处理梯度的噪声问题，在一定程度上减少了随机梯度下降中由于梯度噪声导致的参数更新不稳定的情况。它可以根据梯度的变化情况动态地调整学习率，使模型在训练过程中更加稳定。
    - **缺点**：
      - **超参数引入**：引入了新的超参数，即衰减系数。这个超参数的选择需要根据具体的问题和数据集进行调整，增加了算法的复杂性和调参的难度。如果衰减系数选择不当，可能会影响算法的性能。
      - **依赖全局学习率**：仍然依赖于全局学习率的设置，虽然通过衰减系数对学习率进行了调整，但全局学习率的大小仍然会对算法的性能产生影响，需要进行合理的设置。
7. **Adam 算法**：
    - **优点**：
      - **结合多种优势**：结合了动量法和 RMSProp 算法的优点，既能够加速收敛，又能够自适应地调整学习率。它在处理大规模数据集和复杂模型时表现出了较好的性能，能够快速地收敛到一个较优的解。
      - **易于使用**：相对来说比较容易使用，不需要过多的调参操作就可以取得较好的效果。在很多实际应用中，Adam 算法可以作为一种默认的优化算法，为用户提供了一种方便快捷的模型训练方式。
    - **缺点**：
      - **计算开销较大**：由于需要计算动量项和梯度的累积量等信息，Adam 算法的计算开销相对较大，尤其是对于参数数量非常庞大的大模型来说，计算成本会更高。
      - **可能存在收敛问题**：在某些情况下，Adam 算法可能会出现收敛不稳定的问题，例如在学习率过大或数据集存在特殊分布的情况下，可能会导致模型无法收敛到最优解。

<h3 id='53.描述评估LLM性能的一些技术。'>53.描述评估LLM性能的一些技术。</h3>


以下是评估大型语言模型（LLM）性能的一些常见技术：

### 基于基准测试的评估
- **优点**：标准化程度高，方便在不同模型间进行对比。
- **缺点**：可能无法完全覆盖真实场景中的复杂情况。
1. **通用语言理解评估基准（GLUE）**：
    - **详情**：它包含了多个自然语言理解任务，比如情感分析、文本蕴含、语义相似度等，涵盖了如MNLI（多体裁自然语言推理）、QQP（Quora问题对）等不同的数据集。模型在这些数据集上进行训练和测试，通过计算准确率、F1值等指标来衡量其在自然语言理解方面的性能。例如，在情感分析任务中，判断文本表达的是积极还是消极情感，模型预测准确的比例越高，则说明其在该任务上的理解能力越强。
    - **适用场景**：主要用于评估LLM对文本语义理解、推理等自然语言理解能力，广泛应用于比较不同模型在自然语言处理基础任务上的性能优劣。
2. **超级GLUE（SuperGLUE）**：
    - **详情**：是GLUE基准的升级版，包含了更具挑战性的自然语言理解任务，像BoolQ（判断一个句子是否能回答给定的问题）、ReCoRD（基于知识的阅读理解任务）等。其任务的复杂度和难度更高，要求模型具备更强的语义理解、逻辑推理以及知识融合等能力，通过在这些任务上的表现来综合评估模型性能，指标同样有准确率、F1值等。
    - **适用场景**：旨在测试模型在更高级、更复杂的自然语言理解场景下的能力，用于筛选出在自然语言处理前沿研究领域表现出色的模型，比较先进模型之间的性能差异。
3. **语言模型评估指标（LAMBDA）**：
    - **详情**：从语言生成的流畅性、语法正确性、语义合理性以及与给定上下文的相关性等多个维度，对模型生成的文本进行评估。通过人工标注或者利用一些自动评价工具，给生成的文本打分，综合衡量模型在语言生成任务中的整体质量。例如，对于一个故事生成任务，考察生成故事的情节连贯性、语言是否通顺等方面。
    - **适用场景**：常用于评估LLM在文本生成任务（如故事创作、对话生成等）方面的性能，关注模型输出文本的质量和符合人类期望的程度。

### 人工评估
- **优点**：能考虑到诸多复杂、主观的因素，贴合实际使用场景中的感受。
- **缺点**：耗时费力，成本较高，且具有一定主观性，评估结果可能不够稳定。
1. **直接打分法**：
    - **详情**：招募专业的标注人员或者普通用户，让他们根据预先设定的标准，对模型生成的文本在多个维度（比如内容质量、相关性、可读性等）进行打分，一般采用等级评分制（如1 - 5分）。例如，在评估一个智能写作助手生成的文章时，标注人员根据文章的立意、结构、语言表达等方面给出相应的分数，最后综合所有标注人员的打分情况来衡量模型性能。
    - **适用场景**：适用于各种文本生成任务，尤其是对文本质量要求较高、需要综合考量多个主观因素的情况，像文案创作、学术论文生成等领域。
2. **排序法**：
    - **详情**：向标注人员展示多个由不同模型或者同一模型不同版本生成的文本，让他们按照特定的要求（如质量高低、与主题的契合度等）进行排序。通过分析大量标注人员的排序结果，统计不同模型文本处于较优位置的频率等数据，来判断模型的相对性能。例如，对于几个对话生成模型生成的聊天回复，让标注人员根据回复的合理性、趣味性等因素进行排序，以对比各模型在对话场景下的表现。
    - **适用场景**：常用于对比多个模型或模型变体在特定任务上的优劣，在需要区分细微差异、选出更符合特定需求的模型时较为实用，比如在智能客服系统中选择性能更好的对话模型。

### 自动化评估
- **优点**：高效、可快速批量评估，结果相对客观。
- **缺点**：不能完全替代人工评估，可能无法准确捕捉到一些需要主观判断的质量问题。
1. **BLEU（双语评估替换分数）**：
    - **详情**：最初用于机器翻译任务，衡量生成的翻译文本与参考译文之间的相似度。它基于n-gram（连续的n个单词或字符组成的片段）的匹配情况来计算得分，n-gram匹配度越高，BLEU分数越高，说明生成的文本与参考文本越接近。例如，在将一段英文翻译成中文后，对比生成的中文译文和标准的中文参考译文，计算BLEU分数来评估翻译质量。现在也被拓展应用到其他文本生成任务的评估中。
    - **适用场景**：常用于评估文本生成任务中生成文本与给定参考文本的相似程度，尤其在机器翻译、摘要生成等有比较明确参考文本的场景应用较多。
2. **ROUGE（Recall-Oriented Understudy for Gisting Evaluation）**：
    - **详情**：主要用于评估自动文本摘要系统的性能，通过计算生成的摘要与参考摘要之间的召回率相关指标，来衡量摘要对原文关键信息的覆盖程度。比如有不同的摘要生成模型对一篇新闻文章生成摘要，通过对比它们与人工撰写的高质量摘要之间的ROUGE得分，判断各模型提取关键信息的能力。它有多个变种，如ROUGE-N（基于n-gram的召回率）、ROUGE-L（基于最长公共子序列的召回率）等，从不同角度评估摘要质量。
    - **适用场景**：是文本摘要任务评估的常用指标，用于衡量摘要生成模型能否准确提炼出原文的核心内容，也可部分应用于其他需要关注信息提取和文本浓缩的文本生成场景。
3. **Perplexity（困惑度）**：
    - **详情**：是一种基于概率的评估指标，衡量语言模型对测试数据的预测能力。简单来说，困惑度越低，说明模型对给定文本的概率分布预测越准确，也就意味着模型在语言建模方面表现越好。例如，将一个文本数据集输入到LLM中，计算模型输出的概率分布对应的困惑度值，以此来评估模型是否很好地学习到了语言的规律和模式。
    - **适用场景**：常用于评估语言模型本身的语言建模能力，在比较不同语言模型在同一数据集上的基础性能，或者研究模型在不同训练阶段的性能变化等场景中广泛应用。

### 基于任务的实际应用评估
- **优点**：最贴合实际使用情况，能反映模型在真实需求中的价值。
- **缺点**：评估环境复杂，难以精准归因性能好坏的具体原因，受外部因素影响大。
1. **特定领域任务测试**：
    - **详情**：针对具体的应用领域，如医疗领域的病历分析、法律领域的条文解读等，将LLM应用到相关的实际任务中，通过观察其在完成这些任务时的准确率、召回率、完成时间等具体指标，来评估模型性能。例如，在医疗领域，让模型辅助诊断疾病，根据其对病例中关键信息的提取和疾病判断的正确与否来衡量它在该领域的实用性。
    - **适用场景**：用于考察LLM在各个具体行业和专业领域中的应用能力，帮助筛选出适合特定业务场景的模型，推动LLM在垂直领域的落地应用。
2. **用户体验评估**：
    - **详情**：招募真实用户，让他们在实际使用环境中与搭载LLM的产品（如智能聊天机器人、智能写作助手等）进行交互，收集用户的反馈，比如满意度、易用性、是否解决问题等方面的评价，以此来综合评估模型在实际使用中的性能表现。例如，通过问卷调查、用户访谈等方式，了解用户在使用智能客服系统时，对其回复的及时性、准确性、友好性等方面的感受，进而判断模型的优劣。
    - **适用场景**：关注模型在面向最终用户时的实际体验，适用于各类以LLM为核心的消费级产品的性能评估，旨在优化产品以更好地满足用户需求。 


<h3 id='54.介绍一下LLM使用中的一些伦理考虑'>54.介绍一下LLM使用中的一些伦理考虑</h3>

在使用大型语言模型（LLM）时，存在以下一些伦理考虑：
1. **偏见与公平性**：
    - **数据偏差导致的输出偏见**：训练数据可能存在各种偏差，比如社会、文化、种族、性别等方面的偏见。如果模型在这样的数据上进行训练，可能会在生成的文本中反映出这些偏差。例如，在职业推荐方面，模型可能会基于历史数据中存在的性别差异，对某些职业给出不恰当的性别倾向建议，这会对特定群体造成不公平的影响。
    - **对弱势群体的影响**：偏见的存在可能会进一步边缘化弱势群体。比如，在信息获取和传播方面，如果模型的输出存在偏见，弱势群体可能更难获得准确、公正的信息，从而加剧他们在教育、就业、社会参与等方面的困难。
2. **虚假信息与误导性内容**：
    - **错误信息的传播**：LLM 基于其训练数据生成文本，可能会出现错误或不准确的信息。如果用户过度依赖这些信息，可能会导致错误的决策或认知。例如，在医疗咨询领域，错误的诊断建议或治疗方案可能会对患者的健康造成威胁。
    - **恶意利用生成虚假内容**：恶意用户可能会利用 LLM 生成虚假的新闻、文章、评论等，用于传播谣言、误导公众或进行欺诈等不良行为。这种虚假信息的传播可能会对社会秩序和信任体系造成严重破坏。
3. **隐私与数据安全**：
    - **训练数据的隐私问题**：LLM 的训练需要大量的数据，这些数据可能包含个人的敏感信息，如姓名、身份证号、联系方式等。如果这些数据在收集、存储、使用过程中没有得到妥善的保护，可能会导致用户的隐私泄露。
    - **用户交互数据的安全**：当用户与基于 LLM 的应用程序进行交互时，产生的文本输入、查询记录等数据也可能包含个人隐私信息。这些数据的存储和传输过程需要严格的安全措施，以防止被未经授权的访问或滥用。
4. **责任界定与归属**：
    - **开发者的责任**：LLM 的开发者需要对模型的性能、安全性、伦理合规性等方面负责。他们应该确保模型在设计、训练、优化过程中遵循伦理原则，采取措施减少偏见、错误信息等问题的出现。然而，由于 LLM 的复杂性和不确定性，开发者在某些情况下可能难以完全预见模型的行为和影响，这就需要建立相应的责任界定机制。
    - **用户的责任**：用户在使用 LLM 时也需要承担一定的责任。例如，用户应该对自己使用模型生成的内容负责，不能将其用于违法、违规或不道德的行为。同时，用户在使用 LLM 时也应该保持理性和批判性思维，不盲目相信模型的输出结果。
5. **知识产权与原创性**：
    - **对原创作品的模仿与侵权**：LLM 可以生成与现有作品相似的文本，这可能会涉及到知识产权的问题。如果生成的文本与他人的原创作品过于相似，可能会构成侵权行为。例如，在文学创作领域，使用 LLM 生成的小说、诗歌等作品可能会侵犯原作者的版权。
    - **对创新和原创性的影响**：过度依赖 LLM 可能会抑制人类的创新能力和原创性思维。如果人们习惯于使用模型生成的文本，而不是自己进行思考和创作，可能会导致创新能力的下降，对文化、艺术、科学等领域的发展产生不利影响。
6. **对人类自主性和就业的影响**：
    - **对人类自主性的挑战**：LLM 的广泛应用可能会使人们过度依赖模型的建议和决策，从而降低人类的自主性和判断力。例如，在一些智能助手应用中，如果用户过于依赖模型的回答，可能会失去自己思考和解决问题的能力。
    - **对就业的冲击**：LLM 等人工智能技术的发展可能会导致某些行业的就业岗位减少，例如一些重复性、规律性较强的文字工作可能会被模型所替代。这就需要我们思考如何在技术发展的同时，保障人们的就业权益，提供相应的培训和转岗机会。
7. **对人类价值观和文化的影响**：
    - **价值观的传递**：LLM 的输出可能会传递一定的价值观，如果这些价值观与社会的主流价值观不符，可能会对用户产生不良的影响。例如，模型可能会生成一些宣扬暴力、歧视、极端思想等的文本，这需要我们对模型的输出进行严格的审查和监管。
    - **文化差异的忽视**：LLM 的训练数据主要来自互联网等公开资源，可能会忽视不同文化之间的差异。在跨文化交流和应用中，模型的输出可能会因为缺乏对特定文化背景的理解而产生误解或不适当的内容，这需要我们在使用模型时注意文化的敏感性和适应性。


<h3 id='55.LLM如何处理超出领域或无意义的提示？'>55.LLM如何处理超出领域或无意义的提示？</h3>

大型语言模型（LLM）在面对超出领域或无意义的提示时，通常会采用以下几种方式进行处理：

### 基于概率分布生成回复
- **原理**：
LLM是基于对大量文本数据的学习，构建了语言中词汇、语句等的概率分布模型。当接收到超出领域或无意义的提示时，它依然会依据已学到的概率分布来尝试生成回复。也就是从其“词汇库”和常见的语言组合方式里，按照概率高低挑选词语、组成语句，试图给出一个看起来逻辑连贯的回应。
- **示例**：
比如输入一个无意义的提示“绿月亮在云端跳着方形舞”，模型可能会回复“听起来这是一个很奇特的场景呀，不过不太常见呢，你可以详细说说这背后的故事哦”。它从平时学习到的对话、回应等文本的概率分布中，选择了比较符合对话逻辑、礼貌且通用的语句来进行反馈，尽管实际上并不存在所谓“绿月亮在云端跳方形舞”这样真实合理的情况。

### 参考类似语境或模式回复
- **原理**：
尽管提示可能是超出领域或无意义的，但模型可能会在训练数据中寻找与之在结构、语法、部分关键词等方面有相似性的文本片段，然后参考那些相关文本对应的回复方式来生成内容。它会提取提示中的一些关键元素，去匹配曾学习过的相似语境。
- **示例**：
若输入“飞猪在火星上做算术题”这样荒诞的提示，模型可能联想到训练数据中有关幻想生物在奇特场景里做事的类似表述，回复类似“哇，那可太有意思啦，不知道飞猪做算术题的水平怎么样呀”这样的话语，通过类比其他有想象、奇特元素的文本语境来组织回应内容。

### 给出模糊或笼统的回应
- **原理**：
由于无法确切理解超出领域或无意义提示的准确含义，模型有时会选择给出相对模糊、宽泛、笼统的回答，以此来尽量满足对话交流的基本要求，同时避免给出明显错误或不合理的具体内容。
- **示例**：
当遇到像“彩色的声音如何测量重量”这种违背常理的提示时，模型可能回复“这是一个挺独特的想法呀，不过目前我不太明确具体该怎么去做呢，你可以换个角度再讲讲哦”，通过这种模糊表达来应对难以把握的提示内容。

### 进行一定的纠错提示（部分先进模型具备）
- **原理**：
一些设计更精良、功能更强大的LLM具备一定的文本纠错和提示合理性判断能力。当检测到输入的提示可能无意义或超出正常理解范围时，它们会尝试温和地指出问题所在，引导用户提供更合理的内容。
- **示例**：
对于提示“三角形吃了一个苹果然后变成了圆形”，模型可能回复“你这个描述挺有意思呀，不过三角形属于几何图形，没办法像生物一样吃东西呢，你可以说说更符合常理的情况哦”，主动提醒用户其提示存在不符合逻辑的地方，期望后续能得到更合适的输入。

不过需要注意的是，不同的LLM由于其架构、训练数据、训练方式等的差异，在处理这类超出领域或无意义提示时的具体表现会有所不同，而且它们的处理结果也并非总是完全合理或符合人类期望的，还存在很大的改进和优化空间。 



<h3 id='56.在现实项目应用中，大规模部署LLM有哪些挑战？'>56.在现实项目应用中，大规模部署LLM有哪些挑战？</h3>

在现实项目应用中，大规模部署LLM存在以下挑战：
1. **数据方面**：
    - **数据质量和准确性**：
        - 数据噪声问题，训练数据中可能存在错误、不完整或不一致的信息，影响模型的准确性和可靠性。例如，在文本数据中可能存在拼写错误、语法错误等，这些错误可能会被模型学习到，从而影响生成文本的质量。
        - 数据标注的准确性，对于有监督学习的LLM，准确的标注数据是训练的基础，但标注过程可能存在误差或主观性，导致模型学习到错误的模式。
    - **数据隐私与安全**：
        - LLM的训练和部署通常需要大量的文本数据，其中可能包含用户的敏感信息，如个人身份信息、医疗记录、金融数据等。确保这些数据的隐私和安全，防止数据泄露和滥用，是大规模部署LLM的重要挑战。例如，如果医疗领域使用LLM来辅助诊断，患者的病历数据必须得到严格的保护。
        - 在数据传输、存储和处理过程中，需要采取加密、访问控制、安全审计等多种安全措施，以保障数据的安全性。
    - **数据稀缺性与不平衡性**：在某些应用场景下，可能存在数据稀缺的情况，即缺乏足够的训练数据来训练一个性能良好的LLM。例如，对于一些新兴的领域或特定的行业问题，相关的数据可能较少。另外，数据的不平衡性也可能导致模型的偏差，例如某些类别的数据样本过多或过少，会影响模型对不同类别的学习和识别能力。
2. **模型性能与计算资源方面**：
    - **模型规模与计算需求**：LLM通常具有庞大的模型规模，需要大量的计算资源来支持训练和推理。大规模部署LLM需要强大的硬件基础设施，包括高性能的服务器、GPU集群等，这会带来高昂的成本。例如，训练一个像GPT-3这样的大型语言模型需要数千个GPU运行数周甚至数月的时间。
    - **推理延迟与吞吐量**：在实际应用中，用户对LLM的响应速度有较高的要求，但由于LLM的推理过程是复杂的计算过程，特别是对于较长的文本输入或复杂的任务，可能会导致较高的推理延迟。此外，在大规模部署时，需要同时处理大量的用户请求，如何保证系统的高吞吐量也是一个挑战。
    - **模型压缩与优化**：为了降低计算资源需求和提高推理速度，需要对模型进行压缩和优化。这包括模型量化、剪枝、知识蒸馏等技术，但这些技术可能会影响模型的性能和准确性，需要在性能和资源消耗之间进行权衡。
3. **模型适应性与泛化能力方面**：
    - **领域适应性**：LLM通常是在大规模的通用数据上进行训练的，在应用到特定领域时，可能会出现性能下降的情况。例如，在法律、医学等专业领域，需要模型对专业术语、法律条文、病历等特定的知识有深入的理解，这就需要对模型进行进一步的训练和优化，以提高其在特定领域的适应性。
    - **任务适应性**：不同的任务对模型的要求不同，例如文本生成、问答、情感分析等任务需要模型具有不同的能力和表现。在大规模部署LLM时，需要根据具体的任务需求对模型进行调整和优化，以提高其在特定任务上的性能。
    - **持续学习与更新**：现实世界中的数据是不断变化的，LLM需要不断地学习和更新，以保持对新数据的适应性。然而，大规模部署的LLM模型更新可能会涉及到大量的数据处理和模型重新训练，这需要耗费大量的时间和资源。
4. **可解释性与可靠性方面**：
    - **模型可解释性**：LLM通常是一个黑盒模型，难以解释模型的决策过程和生成结果的原因。这在一些对结果解释性要求较高的应用场景中，如医疗诊断、金融风险评估等，可能会带来问题。用户和监管机构可能对模型的决策过程和结果的可靠性存在疑虑，需要提高模型的可解释性。
    - **模型可靠性与稳定性**：在大规模部署时，模型的可靠性和稳定性至关重要。模型可能会受到各种因素的影响，如硬件故障、软件漏洞、网络问题等，导致模型的性能下降或服务中断。因此，需要建立完善的监控和容错机制，及时发现和解决问题，确保系统的稳定运行。
5. **伦理与法律方面**：
    - **偏见与公平性**：LLM的训练数据可能存在偏见和不平衡，这可能会导致模型在生成文本时产生偏见，对某些群体造成不公平的影响。例如，在招聘、贷款审批等场景中，如果使用存在偏见的LLM模型，可能会导致对某些候选人或申请人的不公平评价。
    - **知识产权与版权**：LLM生成的文本可能会涉及到知识产权和版权的问题。如果模型生成的文本与现有作品相似度过高，可能会侵犯他人的知识产权。此外，在使用LLM时，也需要遵守相关的版权法律法规，确保数据的合法使用。
    - **责任界定**：在使用LLM的过程中，如果出现错误的决策或不良的后果，很难确定责任的归属。是模型开发者的责任，还是用户的责任，或者是其他相关方的责任，需要建立明确的责任界定机制。

<h3 id='57.简单介绍一下LLM在广泛的人工通用智能（AGI）领域中的作用'>57.简单介绍一下LLM在广泛的人工通用智能（AGI）领域中的作用</h3>

大型语言模型（LLM）在通用人工智能（AGI）领域具有多方面的重要作用：
1. **提供强大的语言处理能力**：
    - **语言理解**：AGI 系统需要理解人类的语言，才能与人类进行有效的交流和互动。LLM 通过在大规模语料库上的预训练，学习到了丰富的语言知识和语义理解能力，可以准确地理解文本的含义、语法结构、语义关系等。例如，在智能客服系统中，LLM 能够理解用户的咨询问题，并将其转化为机器可处理的形式，以便后续的回答和解决。
    - **语言生成**：除了理解语言，AGI 还需要能够生成自然流畅的语言文本，以表达自己的想法和回答。LLM 具有出色的语言生成能力，可以根据给定的主题、提示或上下文，生成高质量的文本内容。这在文本创作、智能写作助手、自动报告生成等方面具有广泛的应用，能够大大提高工作效率和创作质量。
2. **支持多模态智能的实现**：
    - **多模态信息融合**：AGI 旨在使智能系统能够同时处理和理解多种不同类型的数据，如图像、声音、文本等。LLM 可以作为一个桥梁，将语言信息与其他模态的数据进行融合和关联。例如，通过将图像的描述文本与图像特征相结合，LLM 可以帮助智能系统更好地理解图像的内容和含义，实现多模态信息的综合处理和分析。
    - **跨模态转换**：LLM 还可以实现不同模态之间的转换，例如将图像转换为文字描述，或将声音转换为文本。这种跨模态转换能力对于实现多模态智能的交互和应用具有重要意义，例如在智能语音助手、图像识别与理解等领域，可以为用户提供更加便捷和自然的交互方式。
3. **促进迁移学习与泛化能力的提升**：
    - **知识迁移**：AGI 系统需要能够在不同的任务和领域之间灵活地应用已有的知识和经验。LLM 的预训练模式使其具有较强的迁移学习能力，可以将在大规模语料库上学习到的通用知识和语言模式，快速迁移到新的任务和领域中。这大大减少了对特定任务的训练数据需求，提高了模型的训练效率和泛化能力。
    - **快速适应新任务**：在面对新的任务和环境时，AGI 系统需要能够快速适应并学习。LLM 可以作为一个通用的基础模型，通过微调或进一步的训练，快速适应新的任务要求。例如，在一个新的领域数据上对预训练的 LLM 进行微调，可以使模型快速掌握该领域的特定知识和语言特点，从而实现对新任务的有效处理。
4. **推动智能系统的发展和创新**：
    - **启发新的研究思路**：LLM 的出现为 AGI 的研究提供了新的思路和方法。研究人员可以借鉴 LLM 的架构、训练方法和优化策略，探索更高效、更强大的智能模型和算法。同时，LLM 的成功也激发了人们对 AGI 的研究热情，推动了相关领域的快速发展。
    - **拓展应用场景**：LLM 的广泛应用为 AGI 的发展提供了丰富的实践经验和应用场景。通过在不同领域的应用，如医疗、教育、金融等，不断探索和挖掘 AGI 的潜在价值和应用模式，为未来 AGI 的全面发展奠定基础。


<h3 id='58.prefixLM和causalLM区别是什么？'>58.prefix LM 和 causal LM区别是什么？</h3>

Prefix LM（前缀语言模型）和Causal LM（因果语言模型）是两种不同类型的语言模型，它们的区别在于生成文本的方式和训练目标。

**Prefix LM**：Prefix LM会基于给定的前缀（即已有的部分文本序列）来预测下一个可能出现的词，适用于文本生成、机器翻译等任务。Prefix LM利用Attention Mask机制，使得其Encoder和Decoder可以共用同一个Transformer结构。在Encoder部分，使用Auto-Encoding（AE自编码）模式，允许前缀序列中的任意两个token相互可见；而在Decoder部分，则实行Auto-Regressive（AR自回归）模式，确保每个token只能看到之前已生成的tokens。

**Causal LM**：Causal LM根据历史历史Tokens来预测下一个Token，适用于文本生成，语言建模等任务。Causal LM只涉及Decoder部分，利用Attention Mask机制来实现Auto-Regressive（AR自回归）模式。

![](./imgs/causalLM.png)


<h3 id='59.什么是LLMs的复读机现象？'>59.什么是LLMs的复读机现象？</h3>

复读机现象（Parroting）指的是LLMs在生成文本时过于依赖复制输入文本，导致缺乏创新和独特性。面对给出的prompt，模型可能会直接复制prompt的一部分或者全部作为输出，而不是生成有意义或者原创的回应。该问题通常出现在几B（亿级）的大模型上，而千亿级的大模型则很少出现这一问题。


<h3 id='60.为什么会出现复读机现象？'>60.为什么会出现复读机现象？</h3>

1.**数据偏差**：LLMs在预训练阶段通常会使用大规模的无标签数据进行训练。若训练数据中包含大量重复文本，模型在生成文本时可能会倾向于重复这些常见的文本。

2.**训练策略缺陷**：LLMs采用自监督学习策略（Self-Supervised Learning）进行训练，即根据历史词句来预测下一个词。这可能导致模型捕捉到数据中的统计规律，而不是学习更深层次的语义和创造性表达。

3.**缺乏多样性的数据**：如果训练数据缺乏多样化的语言表达和语境，模型将难以学习到丰富的多样性和创造性，从而可能导致复读机现象的出现。

4.**模型架构和参数设置**：参数量较小的模型更容易出现复读机现象。


<h3 id='61.如何缓解LLMs复读机现象？'>61.如何缓解LLMs复读机现象？</h3>

1.**多样化训练数据**：在训练阶段，应采用多样化的语料库以训练模型，从而防止数据偏差和文本重复问题。

2.**随机采样策略**：通过引入随机性（temperature,top p和top k指数），如采样不同的词汇或短语，提升生成文本的多样性。

3.**调整搜索策略**：通过调整Beam大小和搜索宽度，可以控制生成文本的多样性和创造性。

4.**后处理和重复过滤**：对生成的文本进行后处理，去除重复的句子或短语，提升文本的质量和多样性。


<h3 id='62.LLMs是否对输入本文的长度有限制？'>62.LLMs是否对输入本文的长度有限制？</h3>

由于嵌入技术的进步，LLMs理论上能够处理任意长度的文本。然而，受限于**计算资源**、模型推理时**梯度消失或爆炸的风险**，以及**上下文理解的局限性**，过长文本往往难以生成准确且连贯的回应。


<h3 id='63.如何让大模型处理更长的文本？'>63.如何让大模型处理更长的文本？</h3>

1.**分块处理**：为减轻长文本对模型内存和计算资源的压力，可将其分块后逐一处理。处理时通过重叠相邻块的部分内容来维持上下文连贯性。

2.**层次建模**：根据文本的层次结构将长文本细分为更小的单元，如段落、句子等。然后再按照层次顺序依次处理。

3.**模型结构优化**：优化注意力机制或者文本嵌入技术，提高长文本的处理效率。


<h3 id='64.为什么需要领域的垂直模型？'>64.为什么需要领域的垂直模型？</h3>

1.**领域的特定知识**：特定领域的大模型需经过针对性训练以掌握行业知识和术语。

2.**语言风格和惯用语**：针对特定领域训练的大模型能更好地掌握其独特的语言风格和惯用语，从而生成更贴合该领域需求的文本。

3.**领域需求的差异**：不同领域对文本处理的需求各异，如金融领域侧重数字和统计数据，法律领域侧重条款和案例分析。

4.**领域数据的独特性**：在数据量较少的领域，通用大模型可能无法得到充分训练。而针对这些领域专门训练的大模型能更有效地利用有限数据，提升模型性能和效果。


<h3 id='65.中文分词有什么难点？'>65.中文分词有什么难点？</h3>

中文文本由于其独特的语言特性，存在以下难点：

- **缺乏明确分隔符**：与英文等使用空格分隔单词的语言不同，中文文本中没有明显的词边界。

- **分词标准**：中文分词缺乏统一的标准，不同的分词方法可能会导致不同的分词结果。比如组合词是否应该分开（“花草树木“ or “花”  “草” “树木”）

- **切分歧义**： **固有型歧义**，分词粒度不同导致的多种切分结果。比如：”华夏子女“可以是”华夏子女“，也可以是“华夏”和“子女”两个词。**偶发型歧义**，相同字元的不同组合导致多种切分结果。比如：“行政处女职员”可以是“行政处”和“女职员”，也可以是“行政”，“处女”和“职员”。**真歧义**，真实存在歧义的句子。比如“人中人梦醒醒醒心不惊”可以是“人中人“ ”梦醒醒“ ”醒心不惊”，也可以是“人中人“ ”梦醒” “醒醒心” “不惊”。中文文本切分歧义频次约为每百字一次，偶发型与固有型占比12:1，真歧义出现概率较低。

- **生词**：也叫未登录词，表示未被词典收录的词。未登录词对分词精度的影响远超歧义切分，识别难度很大。原因在于生词的创造十分迅速、长度不定且无固定边界标志、易与上下文构成切分歧义，且常含外来符号。


<h3 id='66.什么是基于词典的分词算法？'>66.什么是基于词典的分词算法？</h3>

基于词典的分词算法通过一定的策略，将待匹配的字符串与一个庞大的词典进行对比。若匹配成功，则实现分词。根据不同的匹配策略，分词算法分为：**最大匹配法**（见Q21）和**全切分路径选法**。其中，全切分路径选择法通过**N最短路径法**和或者**N元语法模型**将所有可能的切分结果枚举出来，从中选择最优切分路径。

- **N最短路径法**：将所有的切分结果组成有向无环图（切词结果作为节点，词和词之间的边赋予权重），找出权重和的极值路径作为最佳分词路径。例如，可以使用词频作为权重，选择总词频最高的路径作为最佳分词路径。对于句子“研究生命起源”，可能的切分结果有“研究生/命/起源”和“研究/生命/起源”。如果“研究生”的词频低于“研究”和“生命”，则“研究/生命/起源”这条路径的总词频更高，被选择为最佳分词路径。

- **N元语法模型**：这种方法同样基于最短路径，但在构建路径时考虑了词的上下文关系。一元语法模型考虑词的前后一个词，二元语法模型则考虑前后两个词。根据语料库的统计结果，选择概率最大的路径作为最佳分词。例子：对于句子“北京大学生物学院”，一元语法模型可能会考虑以下几种切分方式：“北京大学/大学生/物学院”，“北京/大学生/物学院”，“北京大学/生物/学院”。该模型计算每种切分方式中每个词出现的概率，并尝试找出概率最高的切分作为最佳分词路径。


<h3 id='67.什么是基于统计的分词算法？'>67.什么是基于统计的分词算法？</h3>

基于统计的分词算法将分词问题视为序列标注任务，其中语句中的每个字根据其在词中的位置被标注为B（词开始）、E（词结束）、M（词中间）或S（单字词）。例如，句子“网商银行是蚂蚁金服微贷事业部的最重要产品”经过标注后得到“BMMESBMMEBMMMESBMEBE”，对应的分词结果为“网商银行/是/蚂蚁金服/微贷事业部/的/最重要/产品”。通过统计分析方法获得序列标注结果后，即可实现分词。这类算法通常基于机器学习技术，包括HMM（隐马尔科夫模型）、CRF（条件随机场）、SVM（支持向量机）等。


<h3 id='68.什么是词性标注？难点在哪里？'>68.什么是词性标注？难点在哪里？</h3>

词性作为词语基本的语法属性，是词语和语句的关键性特征。**词性标注**是自然语言处理中的一个基础任务，它涉及给文本中的每个**词汇**分配一个**词性标签**，比如名词、动词、形容词等。其目的是为文本提供更丰富的句法和语义信息。这是后续的语言理解和分析任务的基础，如句法分析、语义角色标注、情感分析等常见的中文标注集是[ICTCLAS词性标注集](https://www.cnblogs.com/chenbjin/p/4341930.html)。

中文词性标注面临**形态变化少**、**一词多词性**、**词性标准不统一**及**未登录词处理**等挑战。

- **形态变化少**是指相较于英文，中文没有词性变化，不能从词的形态来识别词性。例如，英文中的“books”通过复数“-s”后缀可明确为名词，而中文的“书”无论在何种语境下形态都不变。

- **一词多词性**是指中文词既可以做名词也可以做动词等。例如：“基础学科研究”和“研究基础学科”。

- **词性标准不统一**是指LDC标注语料将汉语一级词性划分为33类，但北京大学语料库则划分为26类。

- **未登录词处理**是指无法通过字典直接获取词性。例如：“skrrr”


<h3 id='69.什么是激活函数？什么是梯度爆炸和梯度消失？'>69.什么是激活函数？什么是梯度爆炸和梯度消失？</h3>

***激活函数**：神经网络本身是线性的，激活函数的引入是为了赋予模型处理非线性问题的能力。

**特点和适用场景**：Sigmoid和tanh激活函数能够将输出压缩到(0,1)和(-1,1)的范围内，因此它们适合用于需要概率值处理的场景。ReLU激活函数没有上界限制，可能导致输出值非常大，因此ReLU更适合用于深层网络的训练，因为它能够缓解梯度消失的问题。

**梯度消失问题**：（1）由激活函数引起：sigmoid 和 tanh 激活函数可能导致梯度消失；（2）由矩阵连乘引起：在反向传播中，梯度连乘可能导致梯度消失，尤其是当梯度值小于1.0时。

**梯度爆炸问题**：（1）由矩阵连乘引起：在反向传播中，梯度连乘可能导致梯度爆炸，尤其是当梯度值大于1.0时。

**解决方法**：（1）对于激活函数导致的梯度消失问题，可以使用 ReLU 或其他不会导致梯度消失的激活函数；（2）对于矩阵连乘导致的梯度消失和爆炸问题，可以用梯度截断，残差连接，和归一化技术进行缓解。


<h3 id='70.什么是句法分析？'>70.什么是句法分析？</h3>

句法分析旨在解析句子的句法结构和词汇间的依存关系，为语义分析、情感倾向和观点抽取等自然语言处理任务奠定基础。它主要分为两类：一是分析句子的主谓宾、定状补等句法成分；二是分析词汇间的依存关系，如并列、从属、比较、递进等。

**句法结构分析**：揭示句子的骨架——主谓宾的核心构造，以及定状补等修饰成分，并阐释这些成分间错综复杂的关系。常见的句法结构如下：

![](./imgs/句法结构分析.png)

**语义关系分析**：语义依存关系分析通过识别词汇间的从属、并列、递进等关系，揭示深层语义信息。例如，以下三种表达方式虽句法结构不同，却传达了相同的语义。（a. 春风拂过，桃花笑开了颜。b. 桃花在春风的轻抚下，绽放了笑容。c. 桃花开了。）

**语义关系分析**侧重分析介词等虚词在语句中的作用，而**句法结构分析**则更关注名词、动词、形容词等实词。

常见的句法分析工具有：[哈工大LTP](https://www.ltp-cloud.com/)，[斯坦福Stanford Parser](https://nlp.stanford.edu/software/lex-parser.shtml)

常见的深度学习方法有：RNN，LSTM序列模型等。


<h3 id='71.什么是词向量？'>71.什么是词向量？</h3>

词向量是自然语言处理的基础性工作，其核心包括：**词语编码**和**语义捕捉**。

**词语编码**：将词语转化为计算机可处理的数值向量，解决词语的数字化表示问题。

**语义捕捉**：通过词向量可以表征词语之间的语义关系，如相似性和同义关系等。

词向量模型可通过大规模语料的无监督学习训练获得，常用算法包括CBOW（连续词袋模型）和Skip-gram（跳字模型）。CBOW通过上下文预测中心词，例如利用{“The”, “cat”, “over”, “the”, “puddle”}预测“jump”；而Skip-gram则通过中心词预测上下文，例如“jump”预测“cat”或“over”。CBOW适合小规模语料，具有平滑效果；Skip-gram适合大规模语料，可基于滑窗随机选择上下文词语。Word2vec工具默认采用Skip-gram进行训练。

**模型训练示例**：

```python
# gensim是自然语言处理的一个重要Python库，它包括了Word2vec
import gensim
from gensim.models import word2vec

# 语句，由原始语句经过分词后划分为的一个个词语
sentences = [['网商银行', '体验', '好'], ['网商银行','转账','快']]

# 使用word2vec进行训练
# min_count: 词语频度，低于这个阈值的词语不做词向量
# size:每个词对应向量的维度，也就是向量长度
# workers：并行训练任务数
model = word2vec.Word2Vec(sentences, size=256, min_count=1)

# 保存预训练的词向量模型，下次只需要加载就可以用了
model.save("word2vec_atec")
```

```python
# 先加载已有模型
model = gensim.models.Word2Vec.load("word2vec_atec")

# 进行增量训练
corpus = [['网商银行','余利宝','收益','高'],['贷款','发放','快']] # 新增语料
model.build_vocab(corpus, update=True)  # 训练该行
model.train(corpus, total_examples=model.corpus_count, epochs=model.iter)

# 保存增量训练后的新模型
model.save("../data/word2vec_atec")
```

<h3 id='72.为什么Decoder-only架构成为了大模型的主要框架？'>72.为什么Decoder-only架构成为了大模型的主要框架？</h3>

**避免Encoder的低秩问题**：（1）Encoder-Decoder架构可能存在低秩问题，削弱模型表达能力，而Decoder-only架构无需引入双向注意力，避免了这一问题。（2）在同等参数量和推理成本下，Decoder-only架构因解码参数更多，更具优势。

**更好的Zero-Shot性能**：（1）Decoder-only模型在无标注数据下的Zero-Shot性能优于Encoder-Decoder模型，更适合大规模语料的自监督学习。（2）Encoder-Decoder模型需要多任务微调才能激发最佳性能，而Decoder-only模型无需额外标注数据即可表现优异。

**更适合RLHF（人类反馈强化学习）**：RLHF依赖模型生成结果的排序，而非强监督信号，与Decoder-only的自监督学习范式更契合。

**推理效率高，支持多轮对话**：Decoder-only架构支持KV-Cache复用，对多轮对话更友好。


<h3 id='73.有哪些常见的语言特征抽取网络？'>73.有哪些常见的语言特征抽取网络？</h3>

自然语言处理任务的特点是：输入为一维线性序列且长度不固定，单词或句子的位置关系至关重要，长距离特征对语义理解同样重要。因此，通常采用RNN、CNN和Transformer作为特征抽取网络。

**RNN**是一个可以处理不定长输入、由前向后传递信息的线性网络结构。自然语言处理领域从图像领域借鉴了注意力机制（Attention），并通过叠加网络层深和引入Encoder-Decoder框架，进一步拓展了RNN的能力和应用效果，但RNN结构存在严重的序列依赖，对大模型的并行不友好。

**CNN**通常通过1-D卷积层叠加深度，并采用Skip Connection辅助优化，同时也可以引入Dilated CNN等技术来增强特征捕获能力。CNN的卷积层不仅保留了单词的相对位置信息，还具备强大的并行计算能力，使其在处理NLP任务时高效且有效。

**Transformer**：凭借自注意力机制（Self-Attention）能够捕捉序列中任意两个词之间的关系，解决了长距离依赖问题，同时具备强大的并行计算能力，显著提升了训练和推理效率。

三大网络的能力评测：（a）**语义特征提取能力**：Transformer>>>RNN=CNN；（b）**长距离特征捕获能力**：Transformer>RNN>>>CNN；（c）**综合理解能力**：Transformer>RNN>CNN；(d) **计算效率**：Transformer=CNN>>>RNN。

因此，随着模型规模的快速扩大，RNN将逐渐被淘汰，CNN仍有改进空间，而Transformer将成为主流架构。


<h3 id='74.什么是BERT模型？'>74.什么是BERT模型？</h3>

BERT（Bidirectional Encoder Representations from Transformers）是一种基于双向 Transformer 编码器的语言模型。

其**核心特点**如下：（1）采用双向 Transformer 结构，同时考虑文本序列的左右上下文，实现更深入的语言理解。（2）通过 Mask Language Model （MLM）和 Next Sentence Predicton (NSP) 进行多任务训练，增强 了模型的表达能力。

**输入编码向量**（长度为512）由三个向量相加而成的，包括：（1）位置嵌入（Position Embedding）：编码单词的位置信息，对于理解单词顺序至关重要。（2）WordPiece嵌入：将单词划分成一组有限的公共子词单元，确保单词的有效性和字符的灵活性。（3）分割嵌入（Segment Embedding）：用于标识句子边界，例如在句子对中，第一个句子的嵌入值为0，第二个句子的嵌入值为1。

主要包括**两个任务**：（1）Masked LM（MLM）：约15%的WordPiece Token在单词序列中被随机遮蔽。然后BERT尝试基于序列中其他未被 mask 的单词的上下文来预测被掩盖的原单词。（2）Next Sentence Prediction（NSP）： 使用成对句子作为输入，并预测第二个句子是否是第一个句子的后续句子。

<h3 id='75.什么是Word2Vec工具？'>75.什么是Word2Vec工具？</h3>

**Word2Vec**是Google于2013年推出的自然语言处理工具，其核心功能是将**单词**转化为**向量**表示。通过这种向量化方式，**词与词之间的关系**得以定量**度量**，从而有效挖掘词汇之间的关联性。Word2vec以极快的训练速度盛行一时，但随着**新的embedding模型**（elmo，transformer，gpt，bert，LLMs）的出现开始走向衰落。

![](./imgs/Word2Vec.png)

该工具是一个仅包含单层隐含层的神经网络，其输入和输出均为one-hot编码的词汇表向量。输入层到隐含层的权重即为每个词的分布式表示（Distributed Representation）的词向量。该工具有CBOW和Skip-gram两种训练模型：CBOW适合小数据集，通过上下文单词预测目标单词；而Skip-gram更适合大规模语料，通过目标单词预测上下文。例如，对于句子“There is an apple on the table”，CBOW以（is, an, on, the,, table）为输入，输出apple；Skip-gram则以apple为输入，输出（is, an, on, the, table）。

<h3 id='76.Word2Vec中为什么使用负采样？'>76.Word2Vec中为什么使用负采样？</h3>

在训练神经网络时，通常需要对网络中的所有权重进行微调，以便模型能够更准确地预测训练样本。但是，如果能够只更新网络中的一部分权重，那么可以减少计算量，从而提高训练效率。因此在Word2Vec训练过程中，不是更新网络中所有的权重，而是随机选择少量的（比如5个）负样本，只更新这些负样本对应的权重。


<h3 id='77.什么是Distributedrepresentation?'>77.什么是Distributed representation?</h3>

最早的词向量采用One-Hot编码，其词向量维度等于整个词汇表的大小。对于词汇表中的每个词，将对应位置置为1，其余位置为0，以此表示该词。One-Hot编码如下图所示。

![](./imgs/One-Hot.png)

然而，One-Hot编码存在明显缺陷：词汇表规模庞大，常达百万级别，且每个One-Hot编码相互正交，无法体现词汇之间的相似关系。分布式表示（Distributed Representation）能够有效解决这些问题，其核心思想是通过训练，将原本高维的One-Hot编码映射为低维的词向量。这种低维词向量的维度可根据任务需求在训练时灵活设定，从而在降维的同时保留词汇间的语义关联。Distributed Representation如下图所示。

![](./imgs/DistributedRepresentation.png)

那么有了Distributed Representation就可以分析词之间的关系。一个有趣的发现如下图所示。

![](./imgs/DistributedRepresentation_A.png)


<h1 id='78.你能否概括介绍一下ChatGPT的训练过程？'>78.你能否概括介绍一下ChatGPT的训练过程？</h1>

ChatGPT的训练过程主要包括以下几个步骤：
a.𝗣𝗿𝗲-𝘁𝗿𝗮𝗶𝗻𝗶𝗻𝗴：预训练，大型语言模型在来自互联网的广泛数据集上进行训练，此阶段使模型具备理解语言模式的能力，但尚未具备理解指令或问题的能力。
b.监督微调或者指令微调。模型将用户消息作为输入，模型通过最小化其预测与提供的响应之间的差异来学习生成响应，此阶段标志着模型从仅仅理解语言模式到理解并响应指令的转变。
c.采用人类反馈强化学习 (RHFL) 作为后续微调步骤。

详细介绍如下：
### 预训练阶段
- **数据收集**：收集海量的文本数据，来源广泛，涵盖各种类型的文本，如新闻、小说、论文、社交媒体对话、客服对话、问答对话等，以提供丰富的语言知识和上下文关系.
- **数据预处理**：对收集到的文本数据进行预处理，包括分词、标记化、去除噪声等操作。标记化是将文本转换为模型能够理解和处理的标记序列，还会进行清洗和规范化，如去除特殊字符、标点符号、HTML标签，以及应用停用词去除、词形还原和词干提取等文本清理技术，以简化文本并提高训练效果.
- **无监督学习**：基于Transformer架构，使用大量的文本数据在无监督的方式下进行自回归预测任务，即根据前面的若干个词来预测下一个词，让模型学习语言的概率分布，从而掌握语言的基本规律和各种知识.

### 监督微调阶段
- **收集标注数据**：从测试用户提交的prompt中随机抽取一批，由专业标注人员针对这些prompt给出高质量的答案，形成<prompt, answer>数据对.
- **有监督微调**：使用上述人工标注好的数据对来微调预训练的GPT-3模型，使模型初步具备理解人类prompt中所包含意图，并根据该意图给出相对高质量回答的能力，让模型能够更好地适应特定的任务需求.

### 强化学习微调阶段
- **训练回报模型**：随机抽样一批用户提交的prompt，使用经过监督微调后的模型为每个prompt生成多个回答，人工对这些回答按照相关性、富含信息性、是否包含有害信息等诸多标准进行综合排序。然后，以这些排序结果作为训练数据，采取pair-wise learning to rank模式训练回报模型RM，RM模型接受<prompt, answer>输入，输出评价回答质量高低的回报分数.
- **强化学习微调**：利用训练好的RM模型，根据其打分结果来更新预训练模型的参数，而无需额外的人工标注数据，使模型的答案更接近人类的意图，进一步优化模型的生成策略，减少模型生成内容的偏差和不一致性，提高模型输出与人类期望的对齐程度.

<h1 id='79.大模型中的Reasoning和Inference有什么区别？'>79.大模型中的Reasoning和Inference有什么区别？</h1>

### Reasoning
Reasoning（推理）是指通过逻辑推理、数学计算、逻辑判断等方式，从已知信息中推导出新的知识或结论。在自然语言处理中，Reasoning通常涉及到对文本内容的理解和分析，以便从文本中提取出有用的信息，并基于这些信息进行推理和判断。例如，在问答系统中，模型需要理解问题并从文本中找到相关的答案；在文本分类中，模型需要理解文本的内容并判断其所属的类别。

### Inference
Inference（推断）是指根据已有的证据或信息，对未知事件或现象进行预测或判断。在自然语言处理中，Inference通常涉及到对文本内容的理解和分析，以便从文本中提取出有用的信息，并基于这些信息进行预测或判断。例如，在情感分析中，模型需要理解文本的情感倾向；在文本生成中，模型需要根据已有的文本内容生成新的文本。

### 区别
Reasoning和Inference都是对文本内容的理解和分析，但它们的侧重点不同。Reasoning更侧重于从已知信息中推导出新的知识或结论，而Inference更侧重于根据已有的证据或信息进行预测或判断。在实际应用中，Reasoning和Inference往往会结合使用，以实现对文本内容的全面理解和分析。

DeepSeek R1属于Reasoning模型

<h1 id='80.大模型中外推技术是什么？'>80.大模型中外推技术是什么？</h1>

LLM（大语言模型）的**外推技术**（Extrapolation Techniques）是指让模型在推理阶段处理比训练时更长的上下文（Context）或序列长度的技术。由于大语言模型（如GPT、LLaMA等）在训练时通常有固定的上下文窗口（如4096 tokens），但在实际应用中可能需要处理更长的输入（例如长文档、多轮对话），外推技术旨在扩展模型的上下文处理能力，而不需要重新训练整个模型。

---

### **核心问题与挑战**
- **训练时固定长度限制**：模型在训练时通常使用固定长度的窗口（如4096 tokens），对超出该长度的文本可能表现不佳。
- **位置编码的局限性**：大多数模型依赖位置编码（如绝对位置编码、相对位置编码）来捕捉序列顺序，但这些编码在超出训练长度时可能无法泛化。

---

### **主要外推技术**
以下是几种常见的LLM外推技术：

#### 1. **位置编码优化**
   - **旋转位置编码（RoPE, Rotary Position Embedding）**  
     通过旋转矩阵对位置信息进行编码，相比传统绝对位置编码更具灵活性。RoPE在理论上支持外推，但实际性能可能随长度增加而下降。
   - **ALiBi（Attention with Linear Biases）**  
     在注意力机制中引入线性偏置（随相对距离衰减），使模型在训练时隐式学习对长距离依赖的泛化能力，外推效果较好。

#### 2. **窗口扩展（Window Scaling）**
   - 在推理时通过调整注意力窗口（如缩放注意力分数的温度系数）或动态调整位置编码参数，使模型适应更长的序列。例如：
     - **NTK-aware Scaling**：基于神经切线核（NTK）理论，对位置编码插值以平滑扩展上下文窗口。
     - **动态NTK插值**：动态调整RoPE的基频参数（base frequency），平衡短距离和长距离位置的分辨率。

#### 3. **分块处理（Chunking）**
   - 将长文本分割为多个块（chunks），分别处理后再整合结果。例如：
     - **Transformer-XL**：通过引入循环机制（recurrence）跨块传递隐状态，保留部分长距离依赖。
     - **滑动窗口**：局部注意力结合全局缓存，逐步处理长序列。

#### 4. **微调适配（Fine-tuning）**
   - 在少量长文本数据上对模型进行微调，使其适应更长的上下文。例如：
     - **位置插值微调（Position Interpolation）**：对位置编码进行线性插值，平滑过渡到更长序列。
     - **渐进式扩展**：逐步增加训练时的序列长度，让模型逐步适应。

---

### **外推技术的效果与权衡**
- **优点**：无需完全重新训练模型，成本较低；可快速适配长文本场景。
- **局限性**：
  - 外推性能通常弱于直接训练长上下文模型（如GPT-4的128k窗口）。
  - 可能牺牲短文本的性能（外推过长时位置编码分辨率下降）。
  - 分块处理可能导致上下文碎片化。

---

### **典型应用场景**
1. **长文本生成**：生成小说、论文、长代码等。
2. **多轮对话系统**：保持长期对话历史的连贯性。
3. **文档分析与摘要**：处理超长PDF或书籍。
4. **代码补全**：理解大型代码库的上下文。

---

### **未来方向**
1. **更鲁棒的位置编码**：如可学习的外推位置编码。
2. **稀疏注意力机制**：降低长序列的计算复杂度。
3. **模型架构改进**：如State Space Models（如Mamba）等替代Transformer的结构。

外推技术是当前LLM研究的热点方向，旨在突破模型对上下文长度的限制，推动其在长文本任务中的实用性。

<h1 id='81.大模型中输入token过长，超出模型限制如何截断？'>81.大模型中输入token过长，超出模型限制如何截断？</h1>

当输入token过长，超出模型限制时，大模型会没有回复，原因是因为在配置文件中max_position_embeddings参数是最大处理序列长度，包括了上下文的长度，如果输入超过这个长度，模型则无法输出。
在输如长度超过限制时，需要对输入进行截断，截断方式如下：
```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
text = "你的超长输入文本..."  # 假设长度超过512

# 自动截断到最大长度（默认从右侧截断）
inputs = tokenizer(
    text,
    max_length=512,          # 模型支持的最大长度（如BERT为512）
    truncation=True,          # 启用截断
    return_tensors="pt"       # 返回PyTorch张量（可选）
)
```
max_length长度不能与模型配置文件中的max_position_embeddings参数相等，需要小于该参数，预留可输出token长度


<h1 id='82.什么是DAPO算法？'>82.什么是DAPO算法？</h1>

DAPO（Decoupled Clip and Dynamic sAmpling Policy Optimization）是由字节跳动、清华大学AIR研究院等机构联合提出的强化学习算法，专为**大规模语言模型（LLM）的复杂推理任务优化**而设计。它解决了传统强化学习算法（如PPO、GRPO）在长链思维（long CoT）场景中存在的熵崩溃、训练不稳定、奖励噪声等问题，并在数学推理基准AIME 2024上以Qwen2.5-32B模型取得50分的高分，超越了DeepSeek-R1的47分，且训练步骤减少50%。


---

### 一、**DAPO解决的四大核心问题**
| **问题**          | **传统方法缺陷**               | **DAPO解决方案**               | **效果**                     |
|-------------------|------------------------------|-------------------------------|------------------------------|
| **熵崩溃**         | 对称裁剪限制低概率Token探索     | Clip-Higher（解耦上下裁剪阈值） | 提升生成多样性，熵值稳定      |
| **无效样本梯度消失** | 全对/全错样本导致梯度为零       | Dynamic Sampling动态过滤      | 提升训练效率，收敛速度加快30% |
| **长序列梯度稀释**  | 样本级损失平均稀释关键Token梯度 | Token-Level Policy Loss       | 强化长序列关键推理步骤学习    |
| **过长响应噪声**    | 硬截断惩罚干扰有效推理         | Overlong Reward Shaping       | 长度感知惩罚，训练稳定性提升  |

---

### 二、**关键技术解析**
#### 1. **Clip-Higher：解耦裁剪范围**
- **问题**：传统PPO/GRPO使用固定裁剪范围（如ε=0.2），限制了低概率Token的探索，导致策略快速收敛（熵崩溃）。
- **方案**：  
  - 上裁剪阈值（ε_high=0.28）：放宽低概率Token的探索限制。  
  - 下裁剪阈值（ε_low=0.2）：抑制高概率Token的过度利用。
- **效果**：模型生成多样性显著提升，熵值从骤降转为平稳上升。

#### 2. **Dynamic Sampling：动态过滤无效样本**
- **问题**：当批次内所有样本奖励相同（全对/全错），优势函数为零，梯度消失。
- **方案**：预采样时过滤掉奖励为0或1的样本，仅保留部分正确样本填充批次，确保有效梯度信号。
- **效果**：收敛速度提升30%，训练步数减少50%。

#### 3. **Token-Level Policy Loss：长序列梯度优化**
- **问题**：GRPO的样本级损失平均导致长序列Token梯度贡献被稀释，无法学习关键推理步骤。
- **方案**：按Token计算损失并加权求和（非样本平均），强化长序列中重要Token的学习权重。
- **效果**：响应长度增长更健康，乱码和重复生成减少。

#### 4. **Overlong Reward Shaping：长度感知奖励修正**
- **问题**：硬截断（如长度超限直接惩罚R=-1）误伤有效推理，引入噪声。
- **方案**：  
  - **分层惩罚**：  
    - 安全区（≤ L_max - L_cache）：无惩罚  
    - 缓冲期（L_max - L_cache < 长度 ≤ L_max）：线性惩罚  
    - 超限区（> L_max）：严厉惩罚（R=-1）  
  - **过滤截断样本损失**：屏蔽超长样本的梯度更新。
- **效果**：训练波动减少，AIME准确率提升10%。


<h1 id='883.什么是VAPO算法？'>83.什么是VAPO算法？</h1>

VAPO（Value Augmented Proximal Policy Optimization）是由字节跳动Seed团队于2025年提出的强化学习框架，专为提升大型语言模型（LLM）在**长链复杂推理任务**（如数学证明、代码生成）中的性能而设计。它基于PPO（近端策略优化）框架，通过七项核心技术系统性解决了价值模型偏差、序列长度异构性和奖励稀疏性三大挑战，显著提升了训练效率和推理稳定性。

---

### 一、**VAPO解决的问题与背景**
传统PPO在长链推理任务（如数百Token的数学推导）中面临三大瓶颈：
1. **价值模型偏差**：价值网络从奖励模型初始化时，过度关注序列末尾奖励，导致早期Token的回报估计偏低。
2. **序列长度异构性**：固定参数λ的广义优势估计（GAE）无法同时优化短响应（高方差）和长响应（高偏差）。
3. **奖励信号稀疏性**：二元奖励（0/1）仅在序列末尾提供，导致探索效率低下，易陷入局部最优。

> 💡 **典型场景**：在AIME数学竞赛中，模型需生成多步推导，传统方法常出现“长度崩溃”（输出骤缩）或熵值失控（随机探索）。

---

### 二、**VAPO的七大核心技术**
#### 1. **价值预训练（Value Pretraining）**
- **作用**：消除价值网络初始化偏差。
- **方法**：使用SFT模型生成响应，以蒙特卡洛回报（λ=1.0）预训练价值网络，确保长期回报准确传播至早期Token。
- **效果**：价值模型解释方差提升40%，防止训练崩溃。

#### 2. **解耦GAE（Decoupled GAE）**
- **作用**：分离价值估计与策略优化的权衡。
- **方法**：
  - **价值更新**：λ_critic=1.0（无偏蒙特卡洛回报）。
  - **策略更新**：λ_actor=0.95（降低方差）。
- **效果**：长期依赖建模与策略稳定性兼得。

#### 3. **长度自适应GAE（Length-Adaptive GAE）**
- **作用**：动态平衡长短序列的偏差-方差权衡。
- **方法**：公式：  
  $$\lambda_{\text{actor}} = 1 - \frac{1}{\alpha \times l}$$  
  其中 $l$ 为序列长度，$\alpha=0.05$。短序列λ↓（降方差），长序列λ↑（降偏差）。
- **效果**：长短序列优化误差减少60%。

#### 4. **更高裁剪范围（Clip-Higher）**
- **作用**：缓解稀疏奖励下的熵崩溃。
- **方法**：解耦PPO裁剪阈值（ε_low=0.2, ε_high=0.28），允许低概率Token更大更新空间。
- **效果**：探索熵值提升35%，避免过早收敛。

#### 5. **标记级策略损失（Token-Level Policy Loss）**
- **作用**：解决长序列梯度稀释问题。
- **方法**：损失函数按Token平均而非样本平均，确保长序列中关键步骤权重不被稀释。
- **效果**：长响应（>200 Token）优化效率提升50%。

#### 6. **正例语言模型损失（Positive Example LM Loss）**
- **作用**：强化稀疏奖励下的正确样本学习。
- **方法**：对奖励=1的样本添加负对数似然损失（NLL），模仿其生成模式。
- **效果**：AIME得分直接贡献+6分。

#### 7. **分组采样（Group Sampling）**
- **作用**：提升优质样本利用率。
- **方法**：将高奖励样本分组集中训练，避免被低奖励样本淹没。
- **效果**：训练速度提升2倍，贡献+5分。
