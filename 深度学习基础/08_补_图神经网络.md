- [1.图是什么？](#1.图是什么？)
- [2.什么是图神经网络？](#2.什么是图神经网络？)
- [3.如何进行消息聚合？](#3.如何进行消息聚合？)
- [4.如何进行消息更新？](#4.如何进行消息更新？)
- [5.如何进行重复（获取远端邻居的信息）？](#5.如何进行重复（获取远端邻居的信息）？)
- [6.什么是图卷积网络？](#6.什么是图卷积网络？)
- [7.怎么基于空间构建图卷积神经网络？](#7.怎么基于空间构建图卷积神经网络？)

<h2 id='1.图是什么？'>1.图是什么？</h2>

#### 1. 图的定义

**图**是一种用于表示实体之间关系的数据结构，由两个基本元素组成：

- **节点**：表示实体，如用户、产品等

- **边**：表示节点之间的连接/关系，如关注，购买等关系

![](./imgs/GNN/GNN-1.png)

#### 2. 图的分类

图中两个节点之间既有可能是有向的，比如只能从节点$v_1$到$v_2$；也有可能是无向的，即$v_1$和$v_2$互通。

| 类型 | 特点 | 示例 |
|------|------|------|
| **无向图** | 边没有方向，连接是双向的 | 朋友关系（互为朋友） |
| **有向图** | 边有方向，可以是单向或双向 | 关注关系（A关注B，B不一定关注A） |

#### 3. 图的表示

图通常用$G=(V,E)$来表示，其中$V$表示节点的集合，$E$表示边的集合。

| 符号 | 含义 | 说明 |
|------|------|------|
| $V$ | 节点集合 | 表示图的规模 |
| $E$ | 边集合 | 描述节点间的连接关系 |

其中，每个节点$v_i$通常关联一个特征矩阵$x_{v_i}$，而每条边$e_i=(v_i,v_j)$也关联一个特征向量$x_{e_{ij}}$和邻接值$a_{ij}$。因此，一个图$G=(V,E)$通过节点特征矩阵$X_V$、边特征矩阵$X_E$和邻接矩阵$A$来完整描述节点属性、边属性及拓扑结构。

| 符号 | 维度 | 说明 |
|------|------|------|
| **节点特征矩阵$X_V$** | $R^{N×F}$ | $N$个节点，每个节点有$F$维特征（如用户属性、原子特征）|
| **边特征矩阵$X_E$** | $R^{M×D}$ | $M$条边，每条边有$D$维特征（如权重，关系类型） |
| **邻接矩阵$A$** | $R^{N×N}$ | 表示节点连接关系，$a_{ij}=1$表示两节点相连 |

#### 4.图的特征

- 非欧几里得结构：节点间无序，相邻点数量可变；

- 异构性：图的规模和密度，以及节点特征差异可以极大。

#### 5. 图的应用

a) **社交网络**：节点=用户，边=好友/关注关系
b) **电商推荐**：节点=用户/商品，边=购买/浏览行为
c) **生物信息学**：节点=蛋白质/基因，边=相互作用关系
d) **分子结构**：节点=原子，边=化学键
e) **知识图谱**：节点=实体，边=语义关系


<h2 id='2.什么是图神经网络？'>2.什么是图神经网络？</h2>

#### 1. 图神经网络的定义

**图神经网络（GNN）**是一类专门用于处理图结构数据的深度学习模型。它能够学习**节点**、**边**以及**整个图**的特征表示，从而完成各种预测任务。**图神经网络（GNN）**的核心是通过不断**问邻居节点要信息**，把每个节点变成一个既包含自身特征、又包含周围环境信息的向量，方便后续做预测。

#### 2. 图神经网络的必要性

传统的深度学习模型主要处理规则的网格数据（如图像）和序列数据（如文本、时间序列），但现实中很多数据是非欧几里得结构的图数据，节点数量不固定、邻居关系不规则。

#### 3. 图神经网络的核心机制 - 消息传递

**图神经网络（GNN）**实现**问邻居节点要信息**的核心机制是**邻居聚合**。该流程如下：

a) 聚合：收集邻居节点的信息；

b) 更新：结合自身特征和邻居信息，更新节点的表示；

c) 重复：经过多层传播，节点可以收到更远邻居的信息。

#### 4. 图神经网络的分类

根据聚合和更新方式，图神经网络通常分为以下几类

| 模型 | 全称 | 特点 |
|------|------|------|
| GCN | Graph Convolutional Network | 图卷积，对邻居特征加权平均 |
| GAT | Graph Attention Network | 引入注意力机制，自动学习邻居重要性 |
| GraphSAGE | Graph Sample and Aggregate | 采样邻居，支持归纳学习（新节点） |
| GIN | Graph Isomorphism Network | 理论表达能力强，可区分不同图结构 |
| MPNN | Message Passing Neural Network | 一的消息传递框架 |

#### 5. 图神经网络的主要任务

| 任务类型 | 描述 | 应用示例 |
|------|------|------|
| 节点分类 | 预测节点的类别 | 社交网络的用户分类 |
| 链接预测 | 预测两个节点之间是否存在边 | 推荐系统 |
| 图分类 | 预测整个图的类别 | 分子属性预测 |
| 图生成 | 生成新的图结构 | 场景图生成 |


<h2 id='3.如何进行消息聚合？'>3.如何进行消息聚合？</h2>

**消息聚合**：对于每个节点 $v$，收集其邻域节点 $N(v)$ 的特征信息，生成聚合消息 $m$。

- 通用的消息聚合方式包括**求和**，**均值**和**最大值**。

- **求和**保留邻域的完整信息量（领域大小敏感），适合需要区分不同大小邻域的任务，例如：分子中某原子连接的官能团数量；

- **均值**计算邻域特征的平均水平，适合节点分类任务，例如：社交网络中判断用户类型；

- **最大值**只提取邻域中最显著的特征，丢失大量信息，适合检测是否存在某种特定邻居，例如：缺陷检测。

#### 1. 求和（Sum）

$$
m_i = \sum_{v_j \in N(v_i)} \text{ReLU}(W \cdot x_{v_j} + b)
$$

#### 2. 均值（Mean）

$$
m_i = \frac{1}{|N(v_i)|} \sum_{v_j \in N(v_i)} x_{v_j}
$$

#### 3. 最大值（Max Pooling）

$$
m_i = \max_{v_j \in N(v_i)} \{x_{v_j}\}
$$

---

**符号说明：**

| 符号 | 含义 |
|------|------|
| $x_{v_j}$ | 邻域节点 $v_j$ 的隐藏状态 |
| $N(v_i)$ | 节点 $v_i$ 的邻居集合 |
| $W$ | 可学习的权重矩阵 |
| $b$ | 可学习的偏置向量 |


**注：不同的GNN模型使用不同的聚合函数，会在具体模型架构处讲解**


<h2 id='4.如何进行消息更新？'>4.如何进行消息更新？</h2>

利用聚合得到的消息 $m_i$ 和当前节点的旧状态 $h_i^{(l)}$，更新节点的隐藏状态：

$$
x_{v_i}^{(t+1)} = \sigma(x_{v_i}^{(t)} \oplus m_i)
$$

**符号说明：**

| 符号 | 含义 |
|------|------|
| $x_{v_i}^{(t)}$ | 节点 $i$ 在第 $t$ 层的隐藏状态 |
| $x_{v_i}^{(t+1)}$ | 节点 $i$ 在第 $t+1$ 层的更新后状态 |
| $m_i$ | 从邻域聚合得到的消息 |
| $\sigma$ | 激活函数（如 ReLU、Sigmoid） |
| $\oplus$ | 拼接（Concatenation）或线性变换操作 |


<h2 id='5.如何进行重复（获取远端邻居的信息）？'>5.如何进行重复（获取远端邻居的信息）？</h2>

每一层 GNN 只能聚合直接邻居的信息。通过堆叠多层，信息可以逐层传递，从而获取更远邻居的信息。

$$
x_{v_i}^{(t+1)} = \sigma\left(x_{v_i}^{(t)} \oplus \text{AGG}\left(\{x_{v_j}^{(t)} : v_j \in N(v_i)\}\right)\right)
$$

**符号说明：**

| 符号 | 含义 |
|------|------|
| AGG | 表示消息聚合操作 |

```
第 0 层（初始）：每个节点只有自己的特征
     A ← 只知道自己

第 1 层：A 聚合直接邻居（1-hop）
     A ← B, C, D（A 的直接邻居）

第 2 层：A 再次聚合邻居，但此时邻居 B, C, D 已经包含了它们各自邻居的信息
     A ← B(含E,F), C(含G), D(含H,I)相当于 A 间接获得了 2-hop 邻居（E, F, G, H, I）的信息
````


<h2 id='6.什么是图卷积网络？'>6.什么是图卷积网络？</h2>

#### 1. 图卷积网络（GCN）的定义

**图卷积网络**将**卷积运算**从传统数据（例如图像）推广到**图数据**。其核心思想是学习一个函数映射 $f$，通过该映射图中的节点 $v_i$ 可以聚合它自己的特征 $x_{v_i}$ 与它的邻居特征 $x_{v_j}$（$v_j \in N(v_i)$）来生成节点 $v_i$ 的新表示。

#### 2. 图卷积网络（GCN）与卷积网络（CNN）的联系

当图卷积网络的算法在节点层次运行时，图池化模块可以与图卷积层交错，将图粗化为高级子结构。

```
# GCN
原始图 → 图卷积层 → 图池化层 → 图卷积层 → 图池化层 → ... → 图级表示 → 分类结果(聚合邻居特征) (合并节点)  (聚合邻居特征) (合并节点)

# CNN
输入图像 → 卷积层 → 池化层 → 卷积层 → 池化层 → ... → 全连接层 → 分类结果
           (提取特征)  (缩小尺寸)  (提取特征)  (缩小尺寸)
```

#### 2. 图卷积网络（GCN）的分类

| 类别 | 核心思想 |
|------|----------|
| **基于谱（Spectral-based）** | 从图信号处理的角度引入滤波器来定义图卷积，图卷积操作被解释为从图信号中去除噪声 |
| **基于空间（Spatial-based）** | 将图卷积表示为从邻域聚合特征信息 |


<h2 id='7.怎么基于空间构建图卷积神经网络？'>7.怎么基于空间构建图卷积神经网络？</h2>

#### 1. 图像可以看作特殊的图

![](./imgs/GNN/GNN-2.png)

- **图像**（左）：每个像素是一个节点，像素与相邻像素连接，是一个规则的网络结构，拥有固定的邻居数量。

- **图**（右）：每个节点是一个实体，节点与邻居节点连接，具有不规则的拓扑结构，且邻居数量不固定。

```python
# CNN
# 3×3 卷积核
kernel = [
    [w1, w2, w3],
    [w4, w5, w6],
    [w7, w8, w9]
]

# 对中心像素及其8个邻居进行加权求和
output[i,j] = w1*x[i-1,j-1] + w2*x[i-1,j] + w3*x[i-1,j+1] +
              w4*x[i,j-1]   + w5*x[i,j]   + w6*x[i,j+1]   +
              w7*x[i+1,j-1] + w8*x[i+1,j] + w9*x[i+1,j+1]

# GCN
# 对中心节点及其邻居进行聚合
def graph_conv(center_node, neighbors):
    # 问题：邻居没有固定顺序，数量也不固定！
    aggregated = aggregate([x[n] for n in neighbors])  # sum/mean/max
    output = transform(center_node, aggregated)
    return output
```

#### 2. 基于空间的图卷积核心思想

采用基本的消息传递方式，即**邻居聚合**。

a) 聚合：收集邻居节点的信息；

b) 更新：结合自身特征和邻居信息，更新节点的表示；

c) 重复：经过多层传播，节点可以收到更远邻居的信息。   

PYTHON可以直接采用``` from torch_geometric.nn import GCNConv```调用。

#### 3.多图叠加原理

根据参数共享的机制，分为Composition和Recurrent-based。前者每层有独立参数（W1 → W2 → Wn），后者所有层共享参数（W → W → W）。
